% This is the Reed College LaTeX thesis template. Most of the work
% for the document class was done by Sam Noble (SN), as well as this
% template. Later comments etc. by Ben Salzberg (BTS). Additional
% restructuring and APA support by Jess Youngberg (JY).
% Your comments and suggestions are more than welcome; please email
% them to cus@reed.edu
%
% See http://web.reed.edu/cis/help/latex.html for help. There are a
% great bunch of help pages there, with notes on
% getting started, bibtex, etc. Go there and read it if you're not
% already familiar with LaTeX.
%
% Any line that starts with a percent symbol is a comment.
% They won't show up in the document, and are useful for notes
% to yourself and explaining commands.
% Commenting also removes a line from the document;
% very handy for troubleshooting problems. -BTS

% As far as I know, this follows the requirements laid out in
% the 2002-2003 Senior Handbook. Ask a librarian to check the
% document before binding. -SN

%%
%% Preamble
%%
% \documentclass{<something>} must begin each LaTeX document
\documentclass[12pt,twoside]{reedthesis}
% Packages are extensions to the basic LaTeX functions. Whatever you
% want to typeset, there is probably a package out there for it.
% Chemistry (chemtex), screenplays, you name it.
% Check out CTAN to see: http://www.ctan.org/
%%
\usepackage{graphicx,latexsym}
\usepackage{amssymb,amsthm,amsmath}
\usepackage{longtable,booktabs,setspace}
\usepackage{chemarr} %% Useful for one reaction arrow, useless if you're not a chem major
\usepackage[hyphens]{url}
\usepackage{rotating}
\usepackage{natbib}
\usepackage{listings}
\usepackage{tikz}
\usepackage{xcolor,colortbl}
\usepackage{float}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{subcaption}



\newenvironment{codeexample}[1][htb]
{\floatname{algorithm}{Code Example}%\renewcommand{\algorithmcfname}{Code example}% Update algorithm name
	\begin{algorithm}[#1]%
	}{\end{algorithm}}


\lstset
{ %Formatting for code in appendix
	basicstyle=\footnotesize,
	numbers=left,
	stepnumber=1,
	showstringspaces=false,
	tabsize=4,
	breaklines=true,
	breakatwhitespace=false,
}

% Comment out the natbib line above and uncomment the following two lines to use the new
% biblatex-chicago style, for Chicago A. Also make some changes at the end where the
% bibliography is included.
%\usepackage{biblatex-chicago}
%\bibliography{thesis}

% \usepackage{times} % other fonts are available like times, bookman, charter, palatino

\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}



\title{My Final College Paper}
\author{Your R. Name}
% The month and year that you submit your FINAL draft TO THE LIBRARY (May or December)
\date{May 200x}
\division{Mathematics and Natural Sciences}
\advisor{Advisor F. Name}
%If you have two advisors for some reason, you can use the following
%\altadvisor{Your Other Advisor}
%%% Remember to use the correct department!
\department{Mathematics}
% if you're writing a thesis in an interdisciplinary major,
% uncomment the line below and change the text as appropriate.
% check the Senior Handbook if unsure.
%\thedivisionof{The Established Interdisciplinary Committee for}
% if you want the approval page to say "Approved for the Committee",
% uncomment the next line
%\approvedforthe{Committee}

\setlength{\parskip}{0pt}
%%
%% End Preamble
%%
%% The fun begins:
\begin{document}

  \maketitle
  \frontmatter % this stuff will be roman-numbered
  \pagestyle{empty} % this removes page numbers from the frontmatter

% Acknowledgements (Acceptable American spelling) are optional
% So are Acknowledgments (proper English spelling)
    \chapter*{Acknowledgments}
	I want to thank a few people.

% The preface is optional
% To remove it, comment it out or delete it.
    \chapter*{Preface}
	This is an example of a thesis setup to use the reed thesis document class.



    \chapter*{List of Abbreviations}
		You can always change the way your abbreviations are formatted. Play around with it yourself, use tables, or come to CUS if you'd like to change the way it looks. You can also completely remove this chapter if you have no need for a list of abbreviations. Here is an example of what this could look like:

	\begin{table}[h]
	\centering % You could remove this to move table to the left
	\begin{tabular}{ll}
		\textbf{ABC}  	&  American Broadcasting Company \\
		\textbf{CBS}  	&  Columbia Broadcasting System\\
		\textbf{CDC}  	&  Center for Disease Control \\
		\textbf{CIA}  	&  Central Intelligence Agency\\
		\textbf{CLBR} 	&  Center for Life Beyond Reed\\
		\textbf{CUS}  	&  Computer User Services\\
		\textbf{FBI}  	&  Federal Bureau of Investigation\\
		\textbf{NBC}  	&  National Broadcasting Corporation\\
	\end{tabular}
	\end{table}


    \tableofcontents
% if you want a list of tables, optional
    \listoftables
% if you want a list of figures, also optional
    \listoffigures

% The abstract is not required if you're writing a creative thesis (but aren't they all?)
% If your abstract is longer than a page, there may be a formatting issue.
    \chapter*{Abstract}
	The preface pretty much says it all.

	\chapter*{Dedication}
	You can have a dedication here if you wish.

  \mainmatter % here the regular arabic numbering starts
  \pagestyle{fancyplain} % turns page numbering back on

%The \introduction command is provided as a convenience.
%if you want special chapter formatting, you'll probably want to avoid using it altogether

% The three lines above are to make sure that the headers are right, that the intro gets included in the table of contents, and that it doesn't get numbered 1 so that chapter one is 1.

% Double spacing: if you want to double space, or one and a half
% space, uncomment one of the following lines. You can go back to
% single spacing with the \singlespacing command.
% \onehalfspacing
% \doublespacing


\chapter*{Introduction}
\lstset{language=C++}
\addcontentsline{toc}{chapter}{Introduction}
\chaptermark{Introduction}
\markboth{Introduction}{Introduction}

	\section{Overall motivation}

		In the heterogeneous computing world, there are tools to create software for specific devices. There are also languages which can compile to many different devices (e.g. OpenCL). However, the number of devices is growing, and their characteristics are getting further apart. There will probably never be one language which works for all devices, and all problems. And there is certainly not one available now. So programmers have to write code for different kinds of devices, so that their application can run efficiently. However, programmers have to rely on their knowledge and intuition to figure out what sorts of hardware is appropriate for their application. As the number of different hardware platforms keeps on growing, this puts more and more strain on software developers to become knowledgeable about all sorts of hardware.

		To solve this problem, we suggest making a tool, which can examine CPU code run on a typical workflow, and output suggestions of what kind of hardware different parts of the code can run on.

		This will allow programmers to focus on the hardware platform that is most useful to them reducing search cost. They will still need to learn how to program that hardware, and make their code run efficiently, but at least they will not d all that work to find out that they chose the wrong hardware.

		\subsection{General areas of use cases}

		\subsubsection{Performance-Generality tradeoff}

		General purpose hardware to improve performance is fairly limited in scope. GPUs are broadly understood by the performance programming community, and more accessible tools are improving at a great pace. FGPAs are quickly catching up. And other coprocessors, like Audio and Encryption accelerators seem to lack proper enough generality to be useful in broad cases [CHECK IF THIS IS ACTUALLY TRUE, LOOK FOR PEOPLE WHO HAVE TRIED TO DO THIS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!].

		This is because the tradeoff between generality and performance, which has few points since the software efforts to adopt new hardware is too great to justify too many points.

		One possible use case is reducing the cost of adding more points to this curve. If people can match code to hardware capabilities easily, then the search cost of finding the right parts of the software and finding the right parts of the hardware can be reduced. Lowering the cost of changes along this curve can increase granularity, and perhaps bring a new wave of slightly less general hardware. [TALK ABOUT EITAN'S SMART NICs]

		However, while this use case is intriguing in theory, in practice, it will change nothing until people are already using it,  which is unlikely if it is not very useful. So we need to look for other useful.

		\subsubsection{Power-Performance tradeoff}

		Perhaps the most useful application of this design is helping programmers understand how different hardware platforms will impact power consumption.

		Performance and generality have clear tradeoffs. But when you throw in power into the mix, the hardware world becomes much more daunting.

		In power, the hardware can simply adopt similar software interfaces, but have dramatically different

		\footnote{\cite{Taylor:2013}}

		There are a wide variety of new hardware

		%\subsection{Mainstream approach}

		%Right now, people face the tradeoff between performance and productivity as a fact of life. Except for GPUs, which have mature high level languages, adopting a new hardware platform usually means

		%The reality is that the vast majority of software does not see heterogeneous computing as a viable option. Of course, machines are fast enough that we don't have to worry about most software, only ones that consume lots of resources.

		%So what kinds of software are we looking to help?

		%The advance of scripting languages such as Python, Ruby, and Javascript means that most modern code is running much slower than it could be if written in a more performance centric language, and so improvements in software can dramatically improve performance and power with no changes to hardware. This is the extreme "productivity" side of the tradeoff,  and there is little we can do here.

		%But the advance of these scripting languages also has brought fast CPU-GPU libraries to exploit various hardware platforms. This is a dominant paradigm in scientific computing.

		%The best supported and most popular libraries, like BLAS, have separate implementations for every imaginable hardware platform. This is the extreme "performance" side of the tradeoff, and again, there is little we can do.

		%\subsection{Best approach (mainstream among theorists)}

		%Right now, academics see the most promising route to be changing software models.

		%Existing cross heterogeneous computing languages and paradigms, such as OpenCL, struggle with "performance portability." This means that when the programmer tries to run the code on new hardware, it may run much worse than the original hardware. [CITATION!!!!!!!!!!!!!!!!!!!!!!!!!!!!!]

		%\subsection{Improving mainstream approach}


	\subsection{Note about workload characterization}

		A related field of research is workload characterization. The purpose here is to examine how real software runs on hardware, with the purpose of improving the hardware to run the software faster.

		Our work is related since both fields of study are interested in how real software performs on hardware, what sorts of software can run on what sorts of hardware. This means we will use tools originally intended for workload characterization, particularly binary instrumentation tools.

		However, the fields have a fundamentally different purpose. We are making a tool to help inform a software developer working with a particular software project about the many different kinds of hardware available. Workload characterization is fundamentally making a tool for the hardware developer, working with a particular piece of hardware, and informing them about the many different types of software.

		This fundamental difference will mean that we cannot copy over the same techniques. In particular, hardware developers have to assume that software runs well “as is” as they cannot hope to improve it. However, software developers can change their software to fit the hardware better. So we need to be much more open to changes in the order of execution of parallel instructions, for example, as they will certainly change when moving from CPU to GPU.

		It seems to me at first glance that this will mean that we need much more complete information about the program than the summary statistics that workload characterization researchers use.

		Two examples I can think of right now are:

		\begin{enumerate}
			\item SIMD instruction size: We need to know if the SIMD size expressed by the cpu code is optimal, or if it can be substantially increased by minor changes to the code. This may be the difference of porting it to GPU improving performance or not

			\item Level of parallelism. It is kind of difficult to find natural parallelism in arbitrary code to begin with. But when you consider that certain operations (like reductions) are implemented in a highly sequential manner in single threaded programs, but can be rewritten to run in a massively parallel manner. So ideally, we should be able to figure out if they can be written in this way.  (perhaps by keeping track of associative operations)
		\end{enumerate}

		\subsection{Note about Intel\textsuperscript{\textregistered} Advisor}

		Intel advisor is a very similar tool to what we are using. It's goal is to tell the programmer how they should go about vectorizing and threading their code. This tool can be very effective for both Intel CPUs and Intel Xeon Phi Coprocessors.

		Our end goals are quite separate. Intel's goal is to create an interactive workflow that continuously profiles code on the machine as you optimize it. This is impossible for our purposes, as we will only instrument CPU binary code (this is the only code Intel cares about), and not the code of the machine we are interested in making inferences about.

		But this goal is super important in its own right, and there does not appear to be an open source alternative to intel's tools right now.

		One use case goes like this:

		\begin{enumerate}
			\item
			You are developing code for a system, you profile the code and you find a performance critical section.
			\item You use our tool to understand if this section of code can be parallelized, and what kind of parallelization it can handle (vector, thread, or both).

			\item If the tool tells you it can be parallelized, and what kind, this will allow you to confidently spend time on this code knowing that it will be worth it.

			If the tool tells you it can't be parallelized, then it should give you some indication about why it can't be, and where in the code the problem is arising.

			This should allow the programmer to determine if there is some workaround for the problem.

			possible problems/fixes:

			\begin{enumerate}
				\item Problem: Critical data dependency:

				Possible solution: Perhaps there is an algebraic or algorithmic trick that allows you to parallelize it.

				\item Problem: side data dependency (dependency occurs in a logger, assert, or some other non-critical code)

				Possible solution: There is usually some workaround here. In the worst case, you can just remove the non-critical code.

				\item Data strides of more than 1 (hurts vectorization):

				Possible solution: This can usually be optimized somehow, although sometimes the solution is not trivial.

				\item Non-regular access (vector issues). Similar to previous one, except even less likely to succeed, and even more non-trivial.
			\end{enumerate}

		\end{enumerate}

		Note that while this is useful for developing faster code for a single system, it is also necessary to properly use our tool. Parallelism is a critical input to the analysis, and if it is wrong, because of some minor error that can be fixed easily, then you need to fix it before running the analysis, or else it will make a mistaken analysis.

		And the Intel advisor code involved is very proprietary, and the tools are very expensive (starts at \$1600), and it is not built to be useful for non-Intel processors, or even Intel Integrated Graphics, so it will not be very helpful to achive our goals.

		So we need some sort of analysis separate from Intel Advisor, but which allows for a similar workflow.

	\section{Parts of thesis}

		This thesis has several different parts. Although they also build to a single core purpose, they also have other interesting connections, inspirations and applications.


		High level implementation description

		There will be several parts

		Tool which extracts basic runtime information from actual running code (dependencies, memory addresses, instruction type)
		Library which takes basic info from tool and converts it into hardware properties (vector sizes, pipeline sizes, parallelism)
		Library which converts hardware properties into advice about hardware


	\subsection{Parallelism detection}

		\subsubsection{Applications of parallelism detection}

		Parallelism detection is super useful for many different applications.

		People have suggested using it for determine where to make speculative automatic parallelization. \footnote{\cite{Chen:2004}} There idea here is that the compiler may not be able to prove that the code is parallel, but it may be able to generate code where if there is not a data conflict, or not many, then there is minimal performance loss. Of course, if there are many dependencies, there may be significant performance loss, so the profiler needs to establish a high degree of parallelism on real inputs.

		It is also useful in a tool which tell the software developer what is parallelizable and what is not.[CITATION!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!]
		In larger, complex software, often dependencies are not clearly defined, and manually determining dependencies by sifting through code may be impractical. But dynamic dependency analysis will tell the developer for sure if a block of code can be safely parallelized on the inputs given (it guarantees nothing about other inputs, but this test may be good enough for many developers).

		Then, of course, it is critical for our goal. The common theme among new architectures is trading off single threaded performance for power efficiency or high degrees of parallelism [CITATION!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!].
		So determining whether it can exploit parallelism is critical to establishing whether the decrease in sequential performance is will be sufficiently offset by improved parallelism.

		So we need to detect parallelism. How do we go about doing this? (Chapter 1)

	\subsection{binary instrumentation}
		\footnote{\cite{Luk:2005}}

	\subsection{Hardware identification}

	\subsubsection{Things we should consider from software side}

	\begin{itemize}
		\item Parallelism  (whole section on that)
			\begin{itemize}
			\item Instruction parallelism (super-scalar vs sequential)
			\item Data parallelism
			\item Thread parallelism
			\item Reductions
			\end{itemize}

		\item Memory access pattern
			\begin{itemize}
			\item is it random
			\item does it allow for vector operations
			\item how many 4k blocks does it touch
			\item Does it stride
			\end{itemize}

		\item Instruction composition
			\begin{itemize}
			\item Lots of floating points operations?
			\item Complex integer functions (div,mul) vs simple ones (add, lea)?
			\item
			\end{itemize}

		\item Code pattern
			\begin{itemize}
			\item Do branches taken in a loop diverge? If so, maybe not GPU, if not, then maybe GPU is good.
			\item Is there non-trivial recursion or random access code pointers? If so, then many processors like GPUs will not work.
			\end{itemize}

	\end{itemize}


	\subsubsection{Things we should consider from hardware side}

	\begin{itemize}
		\item Power vs performance need of software (user input)

		\item Overhead from shifting between processors:

		The big problem is that shifting data across different processors, and telling those processors to do things with that data takes a significant amount of overhead. With GPUs, this can take milliseconds (source).

		\item Superscalar pipelines/dispatch

		\item Degree and levels of parallelism in hardware

		\item Cache sizes/ hierarchy/ local-shared memory/

	\end{itemize}

\chapter{Dynamic parallelism detection}

	\section{Introduction}
	
		We need a way of determining parallelism in order to do any useful inference on hardware compatibility. However, recall that our end goals are larger than just that. We are creating a tool that helps the programmer understand their code and helps them improve the algorithms and prototype them to perform better on different hardware without actually the programmer dealing with the hardware. Consequently, we not only want a parallelism tool that figures out which parts of the code can be parallelized or not, but we also want a tool that helps the programmer understand why code might not be parallelizable and perhaps suggests how to fix it. 
		 
		We came to the conclusion that current open source parallelism detection tools do not meet our efficiency and accuracy requirements, and so we built our own tool which does meet our needs. 
		
		This chapter goes over the necessary background for that tool. In section 2, we introduce the type of parallelism we are detecting, and why it is so important. In section 3, we introduce dynamic analysis on loops, a prerequisite for the base algorithm. In section 4, we introduce the base algorithm for detecting it, and the problems with that base algorithm. In section 5, we go over prior work in this area and discuss how it does not suit our needs completely. 
		
	\section{Loop level parallelism}
	
		Parallelism exists in different code structures and at different levels of granularity. Programmers talk about how parallelism manifests itself in code. These include task parallelism (executing different functions in parallel) and  loop parallelism (running different parts of a loop in parallel). Computer architects talk about different parallelism can be exploited in hardware, including instruction level parallelism, thread level parallelism, and memory level parallelism. 
		
		This thesis focuses on loop level parallelism (LLP). This type of parallelism is common, easy to detect, scalable on massively parallel processors, and simple for programmers to reason about. 
		
		\subsection{What is Loop level parallelism?}
		
		We start by introducing some terminology. A loop is a code construct that executes the same instructions over and over on different data. A loop iteration is a single execution of the code in the loop. Two loop iterations are independent if the later iteration does not use data computed by the prior iteration. 
		
		Loop level parallelism exists when every iteration of the loop can be run independently from every other iteration. 
		
		\subsection{Examples of Loop level parallelism}
		
		Here is code that performs in-place vector-scalar multiplication on a vector \texttt{B} with \texttt{n} elements. 
		%The loop executes n times, multiplying a different element of B each time. 
		
		\begin{codeexample}
			\caption{Vector-scalar multiply}\label{vec-scal-mul}
		\begin{lstlisting}
for(int i = 0; i < n; i++){
	B[i] = B[i] * c;
}
		\end{lstlisting}
		\end{codeexample}
		
		This loop is parallelizable. In this loop, each iteration is simply the calculation of \texttt{B[i] = B[i] * c} for some \texttt{i}. Since the iteration is dependent only on the element of \texttt{B} that it also updates, any of these iterations can be executed before any other iteration. So you can compute these iterations in any order without affecting correctness, so the loop is parallelizable. 
		
		In contrast, here is code that performs a cumulative sum. 
		
		\begin{codeexample}
			\caption{Cumulative Sum}\label{cum-sum}
		\begin{lstlisting}
for(int i = 1; i < n; i++){
	B[i] = B[i-1] + A[i];
}
		\end{lstlisting}
		\end{codeexample}
		
		This loop is not parallelizable. For \texttt{i > 1}, \texttt{B[i] = B[i-1] + A[i];} cannot be computed correctly until \texttt{B[i-1]} is calculated, since \texttt{B[i-1]} is used to calculate \texttt{B[i]}. To summarize, the loop must be executed in a particular order, so it is not parallelizable. 
		
		\subsection{Importance of loop level parallelism}
		
		Loop level parallelism is important to this work for four reasons: it is common, scales well on massively parallel hardware, simple for programmers to reason about, and easy to detect. 
		
		Loop level parallelism is common and easy for programmers to reason about. 
		A large amount of work in research and tool development has been devoted to exploiting loop level parallelism. Many parallelism frameworks are focused on loop level parallelism. OpenMP is a C and C++ parallelism framework that allows programmers to parallelize loops (assuming they are parallelizable) on CPUs with a single line of code. 
		
		Loop level parallelism is easy to detect compared to other types of parallelism. Restricting ourselves to this kind of parallelism will give us a simple and fast algorithm.
		Research in automatic parallelization has been focused mostly on loops, mostly because loops are more common and easier to analyze than other parallel code structures. 
		
		Finally, parallel loops often scale well on massively parallel hardware compared to parallel tasks or recursive calls, simply because they often have a large number of iterations.
%		
%		
%		Of course, loops are very common in code. As prior work on loop parallelism has shown, a large number of these loops are parallel. 
%		
%		Loop level parallelism is part of a class of parallelism called massive parallelism. That means that if the loop is reasonably large, it can be scaled to massively parallel hardware like GPUs or other kinds of accelerators. 
%		
%		Parallelizing loops is also very simple for the programmer to reason about. With OpenMP, one can parallelize loops on CPUs with a one line directive. In lower level accelerator code, like OpenCL, it is still relatively easy to translate loops into parallel kernels, just by porting the inner part of the code to an kernel, and run it on all the iterations of the loop in parallel 
%		
%		Lastly, loop level parallelism is easy to detect. The next section will give a simple algorithm to detect this. Other kinds of massive parallelism require more sophisticated analysis. 
%		
		\section{Dynamic Analysis}
		
			There are two general methods to establish parallelism in code: static analysis and dynamic analysis. Static analysis tools parse code like compilers do, analyzing syntax trees of variable assignments and loops. They attempt to establish general properties of the code. Generally, the results of static analysis are true given any possible input. Unfortunately, as code increases in complexity and generality, it becomes impossible to derive precise results in full generality. 
			%Landi proved that static data flow analysis, which is necessary for parallelism analysis, is undecidable for languages like C\footnote{\cite{Landi:1992:USA:161494.161501}}. 
			For a complex semantic property like parallelism, static analysis returns too many false negatives to be useful for practical purposes. 
			
			Dynamic analysis tools examine running processes like an interpreter does, executing a series of instructions on a particular input, while analyzing the behavior and values of the running program. Since a dynamic analysis tool only observes the program on a particular input, it can only establish properties of the code for that specific input. Therefore, dynamic analysis can give false positives for parallelism, meaning a loop might be parallelizable on the particular input, but not on others. To reduce the number of false positives, a complex input that uses many of the features of the program and reflects production use will be more helpful than a simple, contrived input. 
			
		\subsection{Dynamic Loop Detection}\label{s:loopprof-detection}
		
		Dynamic loop level parallelism detection requires a determination of when a loop starts, ends, and iterates in a running program.
		This is a surprisingly challenging problem. 
		Loops have a clean structure in high level languages like C. It is easy for both the compiler and a human to understand what code is inside the body of a loop, and when the loop starts and ends. However, this work focuses on the dynamic analysis of loops. 
		
		Doing dynamic analysis requires us to understand the structure of loops in machine code. Even if the source code is available, there is not always a precise mapping from machine code to source code, as loops may be compiled to different machine code depending on the compiler. Loops in machine code are hard to detect, as they are implemented with "go to" instructions (referenced in this paper as goto). goto instructions allow the code to jump to fixed instructions elsewhere in the code. These instructions implement various functionality like if-else blocks, while loops, and break statements, so their presence does not guarantee a loop.
		
		To demonstrate the difficulty of finding loops in machine code, here is the standard way of translating the above vector-scalar multiply loop into machine code:
		

		\begin{lstlisting}
int i = 1;
while(i < n){
	B[i] = B[i-1] + A[i];
	i++;
}
		\end{lstlisting}
		\begin{lstlisting}
i = 1
if i >= n goto line 8
tmp1 = B[i-1]
tmp1 = tmp1 * c
B[i] = tmp1
i = i + 1
goto line 2
...
		\end{lstlisting} 
		
		Note that even with a single simple loop, there are two goto statements, one for creating the loop, and one for exiting the loop. With nested loops, branches, and break statements, it quickly becomes very difficult to understand where a particular loop starts and how to know that a particular loop terminated.
		So in order to understand loops in machine code, we have to somehow separate loops out from a tangle of goto instructions. 
		
		Luckily, there is prior work for doing just this. We use LoopProf \footnote{\cite{Chen:2004}} code in order to detect the locations of loops in code. In particular, LoopProf tells us where the loop begins and the nesting structure of loops. 
		
		LoopProf uses dynamic analysis in order to determine where loops are. At a high level, it tracks a list of instructions executed in a function. When a goto jumps back to a instruction on that list, then LoopProf concludes that there was a loop, and that its second iteration just started executing. 
		
		However, the LoopProf implementation, generously provided to us by its author, did not contain all of the functionality we needed. In particular, it did not detect the precise ends of loops, which we needed for dynamic dependence analysis. Consequently, we made some additions in order to suit our needs. 
		%Unfortunately, these are technical enough and far enough away from the point of this chapter, that we will not get into more detail about this. 
		
	\section{The baseline algorithm: The pairwise method}
	
		One of the goals of this thesis is to create a tool that identifies loop level parallelism. The literature has several methods to identify this type of parallelism. A particularly simple method is the naive pairwise method. This thesis's implementation will be an optimization of the pairwise method, so the naive version will be described in detail here. 
		
		Recall that loop level parallelism occurs when each iteration of a loop can be executed independently from every other iteration, and that independence means that one iteration does not depend on data computed by another iteration. 
		The pairwise method tracks reads and writes to memory to determine dependences. Each read is considered to be an input to the computation in the loop, and each write is an output. If one loop iteration writes to one location in memory and a later iteration reads from that same location, then that is a dependence that should be recorded as it inhibits parallelism. 
		
		There is an important difference between memory and dependencies. Memory can sometimes be reused without creating dependencies. In particular, consider a location in memory that is written to before it is read in a loop iteration. The data that was previously at that location was overwritten, so the current iteration will not depend on data at that location that was written to in previous iterations. In this case, we say the location is "killed." Reads of killed memory locations are not tracked in a given iteration.
		
		The pairwise method stores two different structures for a particular loop:
		
		\begin{itemize}
			\item history table: A table of reads and writes of previous loop iterations.
			\item pending table: A table of reads and writes of the current loop iteration.
		\end{itemize}
	
		The pairwise method determines dependencies only using this data. The next section illustrates the method with examples. 
		
		\subsection{Simple demonstration}
		
		% cumulative sum repeat
		%		\begin{lstlisting}
		%for(int i = 1; i < n; i++){
		%	B[i] = B[i-1] + A[i];
		%}
		%		\end{lstlisting}
		
		Here is the cumulative sum procedure from the loop level parallelism example section, written to show only the reads and writes to memory.
			\begin{lstlisting}
for(int i = 1; i < n; i++){
	tmp1 = B[i-1]		//READ B[i-1]
	tmp2 = A[i] 		//READ A[i]
	B[i] = tmp1 * tmp2	//WRITE B[i]
}
			\end{lstlisting}
		
		When the loop is executed, the code reads and writes to the arrays. This read and write information is combined with information about how the loop iterates in our analysis. For example, here is how the above code would be processed when executed. 
		
		\begin{lstlisting}
LOOP START
LINE 2 READ B[0]
LINE 3 READ  A[1]
LINE 4 WRITE B[1]
LOOP ITERATION
LINE 2 READ B[1]
LINE 3 READ  A[2]
LINE 4 WRITE B[2]
LOOP ITERATION
...
LOOP END
		\end{lstlisting}
		
		This combined information is the input to the pairwise method. Note that in the implementation, lines will be notated by their instruction addresses, not their line numbers. 
		
		The actual process of the pairwise method can be seen in the tables below.
		When the loop starts, the pending table and history table are empty. As the first iteration is processed, reads and writes are put into the pending table. Right before the first iteration ends, the pending and history tables looks like this:
		
		
		\begin{minipage}{1.0\linewidth}
		\begin{minipage}[t]{.5\linewidth}
				
		\begin{tabular}{ |c|c|c| } 
			\hline
			\multicolumn{3}{|c|}{Pending Table} \\
			\hline
			Instr & Memory & R/W \\ 
			\hline
			2 & B[0] & R \\ 
			3 & A[1] & R \\ 
			4 & B[1] & W \\ 
			\hline
		\end{tabular}
		\end{minipage}%
		\begin{minipage}[t]{.5\linewidth}
		\begin{tabular}[b]{ |c|c|c| } 
			\hline
			\multicolumn{3}{|c|}{History Table} \\
			\hline
			Instr & Memory & R/W \\ 
			\hline
			\hline
		\end{tabular}
		\end{minipage}
		\end{minipage}
		
		The history table is still empty, as there were no previous iterations of the loop. But as the first iteration ends, the pending table is merged into the history table. 
		
		
		\begin{minipage}{1.0\linewidth}
		\begin{minipage}[t]{.5\linewidth}
		
		\begin{tabular}[b]{ |c|c|c| } 
			\hline
			\multicolumn{3}{|c|}{Pending Table} \\
			\hline
			Instr & Memory & R/W \\ 
			\hline
			\hline
		\end{tabular}
		\end{minipage}%
		\begin{minipage}[t]{.5\linewidth}
		\begin{tabular}{ |c|c|c| } 
			\hline
			\multicolumn{3}{|c|}{History Table} \\
			\hline
			Instr & Memory & R/W \\ 
			\hline
			2 & B[0] & R \\ 
			3 & A[1] & R \\ 
			4 & B[1] & W \\ 
			\hline
		\end{tabular}
		\end{minipage}
	\end{minipage}
		
		Right before the second iteration ends, the tables look like this:
			
		\begin{minipage}{1.0\linewidth}
		\begin{minipage}[t]{.5\linewidth}
		\begin{tabular}{ |c|c|c| } 
			\hline
			\multicolumn{3}{|c|}{Pending Table} \\
			\hline
			Instr & Memory & R/W \\ 
			\hline
			2 & B[1] & R \\ 
			3 & A[2] & R \\ 
			4 & B[2] & W \\ 
			\hline
		\end{tabular}
		\end{minipage}%
		\begin{minipage}[t]{.5\linewidth}
		\begin{tabular}{ |c|c|c| } 
			\hline
			\multicolumn{3}{|c|}{History Table} \\
			\hline
			Instr & Memory & R/W \\ 
			\hline
			2 & B[0] & R \\ 
			3 & A[1] & R \\ 
			4 & B[1] & W \\ 
			\hline
		\end{tabular}
		\end{minipage}%
		\end{minipage}%
	
		At the end of the second iteration, the pairwise method will look through the reads of the pending table and see if there are any memory addresses in the history table that are writes. In this case, the history table has a write at address B[1] by line 4, and the pending table has a read at B[1] by line 2. A dependence between line 4 and 2 will be logged, and  the pairwise method correctly identifies that the loop is not parallel. 
		
		\subsection{Nested loop demonstration}
		
		This method also works for nested loops. We will be using the vector-matrix multiplication procedure presented in Code Example \ref{vec-mat-mul} to demonstrate this. In this procedure, the inner loop is not parallel but the outer loop is. The pairwise method will be able to detect this. This section will walk through the execution of the pairwise method on that procedure. The locations of read and write instructions will be referenced using line numbers from Code Example \ref{vec-mat-mul-expanded}.
		
		\begin{codeexample}
			\caption{Matrix-vector multiply}\label{vec-mat-mul}
		\begin{lstlisting}
for(int i = 0; i < output_size; i++){
	sum = 0
	for(int j = 0; j < input_size; j++){
		sum += M[i][j] * x[j]
	}
	y[i] = sum
}
		\end{lstlisting}
		\end{codeexample}
		
		

\begin{codeexample}
	\caption{Matrix-vector multiply with expanded memory accesses}\label{vec-mat-mul-expanded}
		\begin{lstlisting}
for(int i = 0; i < output_size; i++){	 	//outer loop
	sum = 0					//WRITE sum
	for(int j = 0; j < input_size; j++){ 	// inner loop
		tmp1 = x[j]			//READ  x[j]
		tmp2 = M[i][j]		//READ  M[i][j]
		tmp3 = sum			//READ  sum
		tmp4 = tmp1 * tmp2
		tmp5 = tmp3 + tmp4
		sum = tmp5			//WRITE sum
	}
	tmp6 = sum 				//READ sum
	y[i] = tmp6				//WRITE y[i]
}
		\end{lstlisting}
	\end{codeexample}
		
		
		As the outer loop starts, line 2 writes to the memory address of \texttt{sum}. This is put into the pending table of the outer loop. Then the inner loop starts. Empty pending and history tables are created for the inner loop; these tables are separate from the outer loop's tables.
		
		The inner loop is processed very similarly to the cumulative sum example from earlier. A loop dependence on \texttt{sum} between lines 6 and 9 is logged for the inner loop. At the end of the inner loop, the history table will contain:
		
		\begin{tabular}{ |c|c|c| } 
			\hline
			\multicolumn{3}{|c|}{History Table (inner loop)} \\
			\hline
			Instr & Memory & R/W \\ 
			\hline
			4 & \texttt{x[0],...x[input-size-1]} & R \\ 
			5 & \texttt{M[0][0],...,M[0][input-size-1]} & R \\ 
			6 & \texttt{sum} & R \\ 
			9 & \texttt{sum} & W \\ 
			\hline
		\end{tabular}
	
		When the inner loop finishes, its history table is merged into the pending table of the outer loop. Meaning all of the reads of arrays \texttt{x} and \texttt{M[i]} are put in the pending table of the outer loop. In that pending table, \texttt{sum} is written to but not read; since the first access is a write, in the outer loop, \texttt{sum} is considered killed, and no furthur reads or writes to \texttt{sum} will be put in the pending table, including the accesses from in the inner loop, or the access on line 11. 
		%When line 11 is executed, this read of \texttt{sum} is also is not put in the pending table, as \texttt{sum} is still killed. 
		However, the first write to \texttt{sum} on line 2 is still an important access that could cause a dependency in an outer loop, so it still needs to be stored. 
		
		At the end of the first iteration of the outer loop, the pending table will be
			
		\begin{tabular}{ |c|c|c| } 
			\hline
			\multicolumn{3}{|c|}{Pending Table (outer loop)} \\
			\hline
			Instr & Memory & R/W \\ 
			\hline
			2 & \texttt{sum} & W \\ 
			4 & \texttt{x[0],...x[input-size-1]} & R \\ 
			5 & \texttt{M[0][0],...,M[0][input-size-1]} & R \\ 
			12 & \texttt{y[0]} & W \\
			\hline
		\end{tabular}
	
		This table will be merged into the history table of the outer loop, and the process will repeat. Note that there will not be any read after write dependences logged, since \texttt{sum} and \texttt{y[i]} are only written to and \texttt{x[j]} and \texttt{M[i][j]} are only read from. Based on this analysis, the pairwise method concludes that the outer loop is parallel. 
		Intuitively this is true; the outer loop can be parallelized trivially is the memory of \texttt{sum} is duplicated for each thread. 
	
		\subsection{Generalized algorithm}
		
		To generalize this approach to arbitrary nested loops, the pairwise method keeps track of a global loop stack, a stack of pending and history tables for all of the active loops. With this, we can describe a more precise algorithm for the pairwise method. The significant procedures in this outline will be referenced later as specific implementations are described.
		
		\begin{algorithm}[H]
			\caption{Pairwise method}\label{basic-pariwise}
			\begin{enumerate}
				\item When loop $L$ starts, initialize empty pending and history tables for $L$, and push $L$ onto the loop stack.
				
				\item When a memory location $m$ is accessed by instruction $i$, and loop $L$ is at the top of the loop stack,
					\begin{enumerate}
						\item If location $m$ is killed in the pending table, do nothing. SUBTRACT-KILLED
						\item Otherwise, put that information into the pending table of loop $L$. ADD-ACCESS
					\end{enumerate} 
				
				\item When an iteration of loop $L$ ends,
					\begin{enumerate}
						\item Find conflicting memory locations between reads in the pending table and writes in the history table. Report those conflicts. CONFLICT-CHECK
						\item Merge the pending table into the history table. MERGE
					\end{enumerate} 
				
				\item When loop $L$ terminates, 
					\begin{enumerate}
						\item If there is a loop $L^\prime$ below $L$ on the loop stack,
						\begin{enumerate}
							\item Remove entries in the history table of $L$ that are at killed memory locations of $L^\prime$'s memory location. SUBTRACT-KILLED
							\item Merge the history table of loop $L$ into pending table of loop $L^\prime$ MERGE
						\end{enumerate} 
						\item Pop $L$ off the loop stack.
					\end{enumerate} 
				
			\end{enumerate}
		\end{algorithm}
	
	
		\subsection{Naive implementation}
		
		An implementation of the pairwise method simply needs to define the data structures of the pending and history table, and make the capitalized procedures more precise.	
		The naive implementation of the pairwise method uses the following data structures and procedures:
		
		The pending and history tables are implemented as a hash table indexed by memory addresses and which stores sets of instructions. To simplify the procedure, tables for writes and reads are separate, so there are two pending tables, one for reads and one for writes. The table procedures outlined in the pairwise algorithm description are implemented as follows:
		
		\begin{itemize}
			\item ADD-ACCESS: Use the read pending table if the access is a read, otherwise, use the write table. If there is no entry for the memory address in that table, add an entry, and put the instruction from that new access in the set. If there is an entry, add the accessed instruction to the set. 
			\item MERGE: Repeatedly add every access from the input table to the output table using ADD-ACCESS. 
			\item SUBTRACT-KILLED: For a particular access in the history table, check if there is an entry at that location in the write pending table but not the read pending table. If there is, remove the entry. 
			\item CONFLICT-CHECK: Go through every entry in the pending table, and check if it is a read, and if there is also an entry in the history table at that memory address that is a write. If so, then there is a conflict at that location between all the instructions in the two entries in the table. 
		\end{itemize}
		
		
		
	\section{Prior work on optimizing the pairwise method}
		
		Naive implementations of the pairwise method have a severe memory overhead that limits the usefulness of the approach. Kim et al.'s work mitigates this problem by reducing memory consumption through compression. Our work follows the mechanism used in the SD$^3$ paper to reduce memory overhead. This section gives an overview of the memory considerations involved with the pairwise method and prior attempts to fix this by compressing the working memory. 
		
		\subsection{Memory overhead of pairwise method}
		
		The pairwise method often uses more working memory than ordinary systems have, since it has to store information for each memory access in the worst case. The SD$^3$ paper\footnote{\cite{Kim:2010}} mentions commercial tools which use roughly 100x the memory of the program being profiled. The authors tested their implementation of the naive method on 17 SPEC 2006 C/C++ benchmarks. They found that 14 applications used more than 12GB of memory, exceeding the limits of the test system. Out of the 14 benchmarks that used over 12GB of memory to run, 9 used under 100MB of memory under native execution, and memory overhead was over 120x for all 9 of these applications. 
		This suggests that ordinary users will not be able to profile important pieces of software on production inputs, limiting the usefulness of tools using the naive pairwise method.
%		
%		\subsection{Analysis of memory efficiency}
%		
%		The pairwise method is inefficient because it needs to store memory for every unique \texttt{(memory address, instruction, R/W)} tuple in every loop on the current stack of loops. An implementation using standard, unoptimized data structures might take 32 bytes or more to store each of these tuples. A history table storing all of the current working memory may then take at least 8 times the original working memory of the loop (assuming 4 byte word accesses). But perhaps substantially more, since the same memory address might be accessed by different instructions, or memory accesses might be smaller. Then consider that an nested loop may access most of the program's memory, so the history and pending tables of the outer loops will have to store all of this memory as well. 
%		
%		
%		\begin{tabular}{ |c|c|c| } 
%			\hline
%			\multicolumn{3}{|c|}{Sources of memory overhead} \\
%			\hline
%			Source & Estimated Overhead & Overhead Reasonable Range \\ 
%			\hline 
%			Storing tuples instead of memory & 8 & 4-32 \\ 	\hline
%			Multiple instruction access & 3 & 2-20 \\ \hline
%			Nested loops & 3 & 1-20 \\ \hline
%			\hline
%			Total & 72 & 8-1000 \\ \hline
%		\end{tabular}
%	
%		Note that SD3's actual studies showed substantially higher overhead than 72x in general, showing that some of these are underestimates. 
%
%		\subsection{Time efficiency of a compression method}
%		
%		Any compression method should be fast as well as memory efficient. 
%		Ordinary compression methods, such as statistical file compression, have a inverse relationship between space and time. The better the compression ratio, the longer the computation. But this is not always the case. Sometimes, compression can make computations faster, since you operate over smaller datasets. Since the pairwise method is so slow to begin with, additional time overhead is undesirable. So ideally, a compressed implementation of the pairwise method would use the same or less time than the naive implementation. All compression methods suggested have this property. 
%
		\subsection{Stride compression}\label{sse:stride-descr}
		
		The SD$^3$ paper introduces using strides to compress the memory accesses information in the pending and history tables. We present SD$^3$ approach here as the thesis uses a similar approach. 
		%The SD$^3$ paper suggests compressing strided memory accesses. 
		%Strided memory accesses are very common in code.
		
		\subsubsection{What is a stride?}
		
		A stride is a finite range of integers which have a constant interval between elements. It can be stored efficiently as a triple \texttt{(first, last, interval)}. For example, the even numbers from 2 to 20 would be a stride \texttt{(first: 2, last: 20, interval: 2)}.
		
		Memory is often accessed in a strided fashion in code.
		For example, here is code that accesses an array of memory in a strided fashion. Every third element of the array is written to. 
		
		\begin{lstlisting}
for(int i = 1; i < 12; i = i + 3){
	B[i] = 1;
}
		\end{lstlisting}
		
%		Assuming all elements of the array were set to 0, the resulting array would look like this.
%		
%		\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c|c|c| } 
%			\hline
%			B[0] & B[1] & B[2] & B[3] & B[4] & B[5] & B[6] & B[7] & B[8] & B[9] & B[10] & B[11] \\
%			\hline
%			0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 &  0 \\
%			\hline
%		\end{tabular}
		One way to store all these memory accesses would be to store a list of memory addresses for each index \texttt{[1, 4, 7, 10]}.
		But information can be stored more efficiently as a stride: \texttt{(first:1, last:10, interval:3)}. This is more efficient because no matter how long the stride is, it can always be stored in just 3 values, instead of needing more memory to store each new access. 
		
		%\subsection{Components of the SD$^3$ compressed pairwise method}
		
		\subsubsection{Detecting strides}
		
		The SD$^3$ method detects both the existence and interval of strided accesses. 
		Some access patterns are not strided, and if so, the memory accesses should not be treated as strides. When accesses are strided, then the interval of the stride must be calculated dynamically.  
		
		SD$^3$ suggests using an online stride detection algorithm to calculate both the existence and length of strides.
		This algorithm keeps a global data structure separate from the loop stack which keeps track of separate state machines for every instruction. The state machine is organized as follows:
		
		\begin{figure}[h]
			\caption{Stride detector state machine}
			\label{fig:stride-detector}
			\includegraphics[scale=0.6]{stride-detector}
		\end{figure}
		
		This state machine keeps track of the stride length and position of previous accesses, inferring the stride length of the current access. The strong stride-weak stride states allow for short interruptions in otherwise consistent stride pattern. This may be helpful, for example, when accessing the rows of a two dimensional array. In this case, the access pattern may be unpredictable when switching between rows, but the overall stride pattern will remain the same. 
		
		\subsubsection{Data structure for non-strided accesses}
		
		Some memory access patterns are not strides. When this is the case, the SD$^3$ method calls these accesses "points", and puts them in a "point table", which is similar in structure to the pending and history tables in the naive method. These are kept separate from the strides, which are put in the "stride table". There is both a stride and a point table inside each history and each pending table. 
		
		\subsubsection{Data structure for strided accesses}
		
		Operations on non-strided accesses are efficient because the memory location of an access can be looked up quickly using hash tables. However, strides can conflict and be merged even when they don't start or end on the same address, so different data structures are needed. One possible method would just use a list of strides, but the pairwise comparisons needed to do conflict-check would take quadratic time. The SD$^3$ paper suggests use of interval trees to quickly prune the search space to mitigate this quadratic complexity.
		
		An interval tree is an augmented binary search tree that allows searches for intervals, instead of single values. An interval tree can efficiently find all intervals stored inside it which overlap with another interval. 
		
		Take the following tree below (WIKIPEDIA: \url{https://en.wikipedia.org/wiki/File:Example_of_augmented_tree_with_low_value_as_the_key_and_maximum_high_as_extra_annotation.png}), which stores 5 intervals, including \texttt{[20,30)}, \texttt{[10,15)}, and \texttt{[0,1)}. 
		
		\includegraphics[scale=0.5]{interval-tree}
		
		This tree can be queried to find all intervals that overlap with another one, say \texttt{[40,50)} (this will return \texttt{[3,41)}, and \texttt{[29,99)}). Like a ordinary binary search tree, it is self-balancing and allows $O(\log(n))$ inserts and deletes. Finds are $O(m\log(n))$, where $m$ is the number of intervals it finds. 
		
		In the context of strides, each stride would simply be stored  an element in the tree keyed as an interval from first to last, inclusive. Then sets of strides will be looked up with efficiency relating to the total number of strides found, rather than the size of the set of strides. 
		
		\subsubsection{Merging strides}
		
		A simple way to MERGE two sets of strides is to join them together. However, this does not result in any compression. The online detection method generates strides for single accesses, which are immediately put in the pending table. So unless the adjoining strides in the history table are merged together with these strides during MERGE, the memory performance will not improve over the naive version. 
		
		Two strides are mergeable when they can be written as a single stride. For example \texttt{(first:1, last:10, interval:3)} and  \texttt{(first:13, last:22, interval:3)} can be merged, since the stride  \texttt{(first:1, last:22, interval:3)} covers the same elements as the two of them combined. On the other hand, \texttt{(first:2, last:8, interval:2)} and \texttt{(first:13, last:22, interval:3)} cannot be merged since simply extending the bounds of the stride cannot include all of the elements of both strides.
		
		SD$^3$ uses the interval tree described above to find possible candidates for mergable strides, and then checks further. If strides for a particular instruction are consistently not found to be mergable, then no further attempts are made to merge them. 
		
		\subsubsection{Finding overlap between sets of strides}
		
		During CONFLICT-CHECK, there are 3 different types of possible access conflicts: point-point, point-stride, and stride-stride. Point-point conflicts are found exactly as in the naive version, with hash table lookups. Possible point-stride and stride-stride conflicts are found using interval trees. Checking point-stride conflicts efficiently is simple, but checking stride-stride conflicts in constant time is more complex. 
		
		SD$^3$ introduces a new algorithm called Dynamic-GCD which efficiently finds overlap between strides. 
		To under how constant time stride overlap checking is possible, consider infinite length strides. Take two infinite length strides, with stride length $a$ and $b$. Take two points, one from each stride, and calculate the distance between them. Call this distance $c$. Then there is an integer $x$ and $y$ such that $ax + by = c$ iff the strides conflict. This condition is in turn equivalent to $c = 0 \mod (\gcd(a,b))$, where $\gcd(a,b)$ is the greatest common denominator of $a$ and $b$. This test can be done in constant time with the well known Euclidean algorithm for the GCD. Now, finite strides sometimes do not intersect when their infinite counterparts do, so the Dynamic-GCD algorithm gives a similar, but more complex test than the above that uses the Extended Euclidean algorithm to exactly compute intersection for finite strides. 
		
	\section{Bit compression}\label{s:bit-compress}
	
		A strided format is not the only method of compressing memory accesses. Memory accesses can also be compressed effectively using bit arrays. A bit array is simply an array of boolean variables which are stored inside individual bits, instead of in bytes, like ordinary C variables. 
		
		\subsection{Basis of bit compression}
		
		Stride compression is effective because many memory intensive procedures in real applications access memory in a strided fashion. Bit compression is effective because program memory is locally dense, meaning if memory is accessed in one location, nearby memory addresses are usually also valid program memory which will be accessed eventually. This property is virtually guaranteed by most modern operating systems, and so it is not as dependent on the program's structure as stride compression. 
		
		\subsection{Implementation of a bit compressed integer set}	
		
		A sparse set of integers is best implemented using a hash table. A completely dense set of integers is best implemented using a bit array. So a locally dense set will be implemented by a hash table where each item represents a fixed size set of memory. Figure \ref{fig:bit-local-density} shows the the three different types of sets storing the same information. %the difference between what information the three types of sets store. 
		
		
		\begin{figure}[H]
			\caption{Table comparisons}
			\label{fig:bit-local-density}
			\includegraphics[scale=0.8]{BitSetComparison.pdf}
		\end{figure}
		
		
		
\chapter{Implementation}

	\section{Introduction to workload characterization}
		\label{s:workload-characht}
	
		Workload characterization measures how applications use hardware features. %Workload characterization uses dynamic analysis to collect the runtime characteristics of a set of applications. %running on some set of hardware and of the software. 
		Traditionally, hardware researchers are interested in the sorts of hardware characteristics that are important to application performance. They use workload characterization to understand how trade-offs in hardware design would affect various applications' performance. To accomplish this, they collect metrics on the applications they want to optimize. These metrics inform decisions about the of hardware trade-offs hardware engineers are considering. 
		
		We are interested in a related question: the application characteristics that can help the programmer choose between different established hardware platforms. We use workload characterization to understand the characteristics of single threaded CPU programs that are indicators of performance on other general purpose hardware, such as GPUs. We gathered applications and platform independent metrics that indicate important differences between the different hardware platforms. 
		
		\subsection{Important hardware characteristics under consideration}
		
		This thesis identifies several different hardware features as important, and attempts to capture in metrics. 
		
		\subsubsection{Scale of Parallelism}
		
		GPUs will not be faster than CPUs unless there is a high degree of parallelism, and substantial work to be done. GPUs have hundreds of cores, where CPUs at most two dozen or so. However, CPU cores are much faster. So GPUs will not be effective unless the work can be split over hundreds of threads. In addition, each kernel call must do substantial work, because the overhead of calling kernels is sometimes very high.  
		
		We suggest using the number of iterations in parallel loops, as well as the number of instructions each of those iterations contains to measure the scale of parallelism.
		% memory must be transfered back and forth between In addition, if the computation is extremely simple, say,  sometimes the cost of tran
		
		%GPUs are only useful when the algorithm can be divided into many threads, each of which has substantial content.
		
		%GPUs can run hundreds of threads have hundreds of are only useful when 
		
		
		\subsubsection{Predictability of memory accesses}
		
		
		GPUs have global memory that is optimized for high bandwidth and high latency. Predictable accesses such as strided memory can be streamed efficiently, while unpredictable memory accesses suffer from high latency, and stall the instruction pipeline. 
		%[CITATION???] possibly file:///C:/Users/weepi/Documents/thesis/papers/CCWS.pdf 
		In contrast, CPUs are much better at handling unpredictable memory, since they have much larger caches. % also benefit from predictable memory accesses, but to a lesser extent, since the memory structure is optimized towards lower latencies. 
		
		We chose to capture this performance difference by measuring the number and length of strided accesses. 
		We use the same stride detector code as in our pairwise method (see section \ref{sse:stride-descr}.2 for an design). 
		
		% Then, we simply count the number of accesses which are of certain stride lengths, including zero length strides, and those that cannot be detected as strides. 
		
		\subsubsection{Parallelism Model}
		
		At a low level, GPUs execute threads in groups called warps. Warps have an SIMD (single instruction multiple data) model of execution, meaning that each thread in the warp executes instructions in lock-step. This implies that different threads in the same warp cannot execute different branch paths at the same time \footnote{\cite{Fung:2007:DWF:1331699.1331735}}. This performance hazard is called  branch divergence. Handling branch divergence is time consuming on GPUs, so performance suffers significantly when branch paths differ within a warp.
		
		Meanwhile CPUs have a MIMD (multiple instruction multiple data) model, where each thread executes code independently. CPUs are also equipped with advanced branch prediction which often reduces the cost of branches to nearly zero. 
		
		To account for this performance difference, we collected a count of the number of conditional branches encountered, and the number of time the path of that branch changed. 
		
		\subsubsection{Inter-thread vs Intra-thread memory sharing}
		
		CPUs theads have much larger cache capacity than GPU threads, since there are simply many more GPU threads active at a given time.\footnote{\cite{Jia:6835938}} This means that GPU threads cannot rely on caches to exploit temporal locality to the same degree that CPU threads can. Instead, GPU caches effectively exploit cross-thread locality. 
		
		To capture this, we measure the amount of memory shared between loop iterations vs reaccessed within a single iteration.
		We construct maps of the memory footprint of the loop and each of its iterations. From these memory footprints, we calculate the number of unique memory bytes accessed in each iteration as well whole instance. %From that, the number of shared bytes between threads 
		
%		
%		We capture this 
%		
%		CPU caches are designed to exploit temporal locality. If some piece of memory is accessed, and it is accessed again soon afterwards, the second access should be fast.
%		
%		
%		
%		These caches are quite large, typically 32kb for L1 cache, and 256kb for the L2 cache, so they allow for long temporal locality. 
%		
%		GPU caches and shard memory are not designed to capture long temporal locality. Caches are small, and are shared between many threads. 
%		
%		\footnote{\cite{Rogers:2012:CWS:2457472.2457487}}
%		
%		GPU caches 
%		
%		Global memory is expensive to access on both CPUs and GPUs. CPUs have a small number of cores, each with their own cache. These caches are optimized for temporal locality, i.e. the same memory being used across time.
%		
%		GPUs have hundreds of small cores, so they do not have their own cache. Instead, there are two different hardware features which both allow for fast access of memory which is shared across threads in a single timestep. One way is the shared memory, a different kind of memory in its own address space for local work-groups.
%		
%		the groups of cores have shared memory caches to allow for . 
%		
%		GPUs have shared memory and caches which allow for efficient cross-thread memory reuse assuming the shared data is reasonably small. 
%

		%\subsubsection{Arithmetic intensity}
	
	\section{Metrics}
		
		An important contribution of this paper is using a metric of parallelizability in workload characterization. 
		We created metrics in order to make use of this metric. 
		In particular, since we evaluate loop level parallelizability, we chose to collect all other metrics at the loop level. 
		
		We chose metrics based on the main differences between CPUs and GPUs. 
		
		\begin{tabular}{ |p{6cm}|p{7cm}| }
			\hline
			Metric & Relevant hardware characteristic(s) \\
			\hline \hline
			Number of iterations & Scale of Parallelism \\
			\hline
			Instruction count of loop iterations  & Scale of Parallelism \\
			\hline
			Number of writes/reads & ??? \\
			\hline
			Bytes accessed per instance & Memory??? \\
			\hline
			Total memory footprint of loop & Memory???  \\
			\hline
			Shared memory footprint between loop iterations & Memory??? \\
			\hline
			Strided accesses  & Latency vs. bandwidth \\
			\hline
			Number of branch changes & Warp divergence and branch prediction \\
			\hline
		\end{tabular}
	



	\section{Metrics Collection}
	
		To collect loop level metrics we created a analysis tool powered by Intel Pin.\footnote{\cite{Luk:2005}} This tool uses loop information generated by LoopProf (LoopProf is introduced in section \ref{s:loopprof-detection}). It matches information about specific loop iterations and instances by storing stacks of loop information. 
		
		The parreleism detector is similarly build as a Pin Tool. 
		
		Then to run these on a given application with certain inputs, the application is run three times:
		
		\begin{enumerate}
			%\item Run the workload natively to check native memory consumption
			\item LoopProf evaluates the application's loop starts and ends
			\item Using that loop information, collect the loop dependent metrics of the application
			\item Using the same loop information, collect loop parallelizability information. 
		\end{enumerate}
		%		\subsection{Loop size metrics}
		%		\subsubsection{Number of iterations per loop instance}
		%		\subsubsection{Length of loop iteration by instruction count}
		%		\subsection{Memory metrics}
		%		\subsubsection{Number of reads/writes}
		%		\subsubsection{Bytes accessed per instance}
		%		\subsubsection{Total memory footprint of loop}
		%	
		
	\section{Workloads}
		
		To distinguish between applications that are best run on CPUs or GPUs, we first needed to get a good sample of programs with different characteristics. Some of these should be geared towards CPUs, and others should good cases for GPU acceleration. To get a wide variety of applications, we gathered applications from two different benchmarks: Rodinia and Parsec.
		
		\subsubsection{Rodinia}
		
		Rodina is a benchmark designed to compare CPU and GPU performance on different applications. The Rodinia applications are chosen to reflect the Berkley 7 dwarves of scientific computation.\footnote{\cite{Che:2009}} All of these applications are parallelizable to some degree, and most of them are effective on GPUs. 
		
%		Colella, Phillip. Defining software requirements for scientific computing. Slide of 2004 presentation
%		included in David Patterson’s 2005 talk, 2004. URL http://www.lanl.gov/orgs/
%		hpc/salishan/salishan2005/davidpatterson.pdf.
%		\footnote{\cite{Che:2010}}
		
		For each algorithm, Rodinia has a CPU implementation threaded using OpenMP. Since our tool does not support multithreaded applications, we removed the OpenMP compilation flag, and removed calls to the OpenMP library such as \texttt{omp\_get\_thread\_num()} from the source code. 
		
		\subsubsection{Parsec}
		
		Parsec is a benchmark for multi-threaded applications on CPUs.\footnote{\cite{Bienia:2008}} It has a more diverse range of applications than Rodinia, including computer vision and textual analysis as well as traditional scientific computing applications.
		
		Parsec was more difficult to get running on our system. There were some applications which were incompatible with C++11, and did not compile, and many others which could not be altered to run single threaded without substantially changing the structure of the code, and so they could not be run on our analysis tools.  
		

	
	\section{Stride Compression Implementation}
	
		The code that Kim et al. wrote to implement the SD$^3$ algorithm was not made publicly available, so it is closed source, and we cannot use it to detect parallelism. To achieve the goals of this thesis, we still needed to detect parallelism without running out of system memory, so we chose to implement the algorithm ourselves. As we had limited time to implement it, we decided to make changes to the algorithm for simplicity. In particular, we thought implementing interval trees ourselves would be error-prone, and we could not find an open source C++ implementation, so we chose to use other data structures. 
		In addition, the details of the Dynamic-GCD algorithm proved difficult to work out, so we used other techniques for finding stride conflicts.
		These were key features to ensure efficiency in the original SD$^3$ algorithm, so more changes had to be made to improve efficiency. In particular, a slow conflict-check method was used in our implementation, so optimizations were introduced that minimize the number of conflict-check operations that occur. 
		
		\subsection{Algorithm and data structure overview}
		
			%The key data structures and their use 
			
			First, a high level overview of the main data structures and objects will be introduced, and later the specific data structures and algorithms will be detailed. 
			
			At the highest level, a program has a single LoopStack, which contains a stack of LoopInstances. LoopInstances store all information about a particular active loop, including the pending and history tables.	Separate from the LoopStack, there is a StrideDetector for each instruction that accesses memory. 
			
			There are three main data structures which are central to the algorithm. The purpose of these data structures is outlined here, and the details will be explained later. 
			
			\begin{itemize}
				\item StrideTable: This table stores strided accesses and their instructions. Strides can be added efficiently to this table.
				\item PointTable: This table stores non-strided accesses and their instructions. 
				\item AddressSet: This structure is used as a set of memory addresses, without instructions. It has ordinary set operations, such as union, intersection, set subtraction, and inserting elements. It is used as an optimization only, as it stores strictly less information that the point and stride tables. 
			\end{itemize}
			
			These data structures store the content of the pending and history tables, but reads and writes are stored separately. See Figure \ref{fig:loopinstance} for how these tables fit inside the LoopInstance. 
			
			\begin{figure}[H]
				\caption{LoopInstance Layout}
				\label{fig:loopinstance}
				\includegraphics[scale=1.0]{LoopData.pdf}
			\end{figure}
			
			%Given these data structures, new memory accesses are processed as described in Algorithm \ref{new-memory-acc}, loop iteration endings are described in Algorithm \ref{new-loop-iteration}, and loop termination is described in Algorithm \ref{loop-termination}.
			
%			
%			\begin{algorithm}
%				\caption{Finishing a loop iteration}\label{new-loop-iteration}
%				When an iteration of loop $L$, with LoopInstance $I$ finishes,
%				\begin{enumerate}
%					\item Merge pending table contents into history table
%					\begin{enumerate}
%						\item Add every point and stride in the pending table into the corresponding point and stride tables of the history table
%						\item Union the pending AccessSets into the corresponding history AccessSets.
%						\item Clear the entire PendingTable.
%					\end{enumerate}
%					\item Check for conflicts between the pending and history tables
%					\begin{enumerate}
%						\item Determine if the iteration can be run in parallel by checking the AccessSets. if the intersection of the pending table's read AccessSet and the history table's write AccessSet is empty.
%						\item If the loop iteration is not parallel, find the pairs of instructions that conflict by
%						\begin{enumerate}
%							\item Get the list of all points/strides in the read pending table, and compare it the list of all points/strides in the write history table, and find a list of pairs of overlapping points/strides.
%							\item Filter out the points/strides which are not actually conflicting.
%						\end{enumerate}
%					\end{enumerate}
%				\end{enumerate}
%			\end{algorithm}

%
%			\begin{algorithm}
%				\caption{Upon loop termination}\label{loop-termination}
%				
%				When loop $L$ terminates, with LoopInstance $I$ on the top of the LoopStack, and LoopInstance $I^\prime$ is the next highest on the stack,
%				\begin{enumerate}
%					\item Remove all strides and points from the history table of $I$ which are killed by the pending table of $I^\prime$.
%					\item Merge history table contents into pending table.
%					\item Pop $I$ off the LoopStack.
%				\end{enumerate}
%			\end{algorithm}
%		
		\subsection{Adding accesses}\label{s:add-accesses}
		
		Algorithm \ref{new-memory-acc} describes the high level description of how new memory accesses are put in the pending table. % a lot of details out. These details will be explained here. 
		
		\begin{algorithm}
			\caption{New memory accesses}\label{new-memory-acc}
			When a memory location $m$ is accessed by instruction $i$, and LoopInstance $L$ is at the top of the loop stack,
			\begin{enumerate}
				\item If the memory location $m$ is killed by the pending table's AccessSet, do nothing, and skip the rest of the steps. 
				\item Find the loop-independent stride detector that corresponds to instruction $i$, and use it to calculate whether $m$ is a stride, and if so, its stride distance $d$. 
				%\item Get the point table, stride table, and AddressSet of LoopInstance $L$ corresponding to the accessmode of $L$.
				\item If $m$ is a stride with stride distance $d$ then add it to the appropriate stride table.
				\item If $m$ is not a stride, add it to the appropriate point table. 
				\item Add the bytes accessed by $m$ to the appropriate AddressSet.
			\end{enumerate} 
		\end{algorithm}
		
		
		\subsubsection{Stride Detection}
		
		The StrideDetector is implemented to match the original SD$^3$ design as closely as possible. The state machine matches Figure \ref{fig:stride-detector}.
		
		\subsubsection{Adding Points to the PointTable}
		
			Points (non-strided accesses) are added to the PointTable during new memory accesses and merges. The PointTable is designed so that redundant points are not duplicated in the table. 
			
			The PointTable is simply a set of Points keyed on instruction address and memory address see figure \ref{fig:point-table}. When a point is added, its location in the set is checked. If there is an element there already, nothing happens, otherwise the point is added to the set. 
			%Recall that Points are specified by instruction address, memory address, and access mode (read or write). 
			%In Figure \ref{fig:point-table}, you can see that the a Point is stored for each instruction and memory address. 
			
			\begin{figure}[h]
				\caption{Point Table}
				\label{fig:point-table}
				\includegraphics[scale=0.8]{point_data.pdf}
			\end{figure}
		
			
			%The data structures allow for a simple and efficient merge operation. The bit set is merged by an in-place union operation. For the point and stride tables, the operation simply calls \texttt{extract} to collect all the elements from the source table, and adds them one at a time to the destination table. 
			
			%The add operation is the only non-trivial operation. For points, it is straightforward: add the point if it is not yet there. This works because a point's location in the table uniquely specifies its value. 
			
			
		
		\subsubsection{Adding Strides to the StrideTable}
			
			Strides also added to the StrideTable during new memory accesses and merges. 			
			When possible, strides need to be merged into existing strides in the table when being added. This is critical for compression (see section \ref{sse:stride-descr}.5). The data structure is designed to make this process simple and efficient. 
			
			At a high level, the StrideTable is a unordered set of ordered sets of strides (see Figure \ref{fig:stride-table}). The outer set ensures that a particular inner set contains only strides which are mergeable if they adjoin. The inner ordered set is used to check if the strides do adjoin, in which case, they are mergeable.
			
			\begin{figure}[h]
				\caption{Stride Table}
				\label{fig:stride-table}
				\includegraphics[scale=0.8]{stride_data.pdf}
			\end{figure}
			 
			
			As shown in Figure \ref{fig:stride-table}, the outer set is keyed on the instruction address, the stride distance, and the stride start modulo the stride distance. Strides which have these criteria in common are mergeable if they adjoin.  This is best explained with examples:
			
			\begin{tabular}{ |c||c|c|c| } 
				\hline
				Stride \# & Stride Distance & Stride start & Stride last \\ 
				\hline
				\hline
				1 & 2 & 4 & 8 \\ \hline
				2 & 3 & 4 & 13 \\ \hline
				3 & 2  & 5  & 9 \\ \hline
				4 & 2  & 0  & 2 \\ 
				\hline
			\end{tabular}
		
			In the table above, stride 1 and 4 are mergeable, while no other pair of strides are. It should be clear that stride 2, with a different stride distance from the rest, will not be mergeable with the others except in edge cases. Stride 3 is not mergeable because the elements don't not match up with the other strides. In contrast, note that if the ends of stride 1 and 4 are extended, they could become the same stride. It turns out that this idea of boundary extension is captured perfectly with the stride start modulo the stride distance. 
			% These criteria ensure that all the strides in the inside set of strides which are all mergeable. 
			
			Now, the inside set needs to merge all adjoining strides. This problem is exactly the problem of merging of overlapping intervals, which is already well understood. But to be complete, a solution outline will be detailed here. % union of intervals, and is a much more well known pronl
			
			The inside set is ordered by the first element in the interval (which just happens to be a stride). Overlapping intervals are always merged, there will never be overlapping intervals in the set. This property will be used to guarantee efficiency.
			
			To see how this can be done, consider the below example, where there are three intervals in the table, and one intervals being added. The first intervals cannot be merged with the added intervals, and the other two can. 
			
			\newcommand{\bcl}{\cellcolor{black!50}}
			\begin{tabular}{ |l|l| } 
				\hline
				Current intervals & (1,3),(4,5),(7,9) \\ 
				\hline
				Added interval & (4,6) \\ 
				\hline
			\end{tabular}
			
			Since intervals in the table will never overlap, the interval directly below the added interval (the (1,3) interval) is the only possible interval that starts before it, and also could overlap.% In the example, this is (1,3) interval. 
			
			The intervals above the added interval (in this case, (4,5) and (7,9)) can be efficiently checked because their first element will be less than the added interval's last element, and the intervals are ordered by their first item. %Then, it will merge every stride which starts after it starts, but ends before the added stride ends.
			
			%Two strides are possibly mergeable if they have the same instruction address, the same stride distance, the strides would be the same if they were infinite length. Referring back to Figure \ref{fig:stride-table}, these are exactly the strides which have the same value in the outer set, and so are placed in the same binary search tree. 
			
			%These criterion guarantee that any adjoining strides are mergable. The inner binary search tree is used to check if the strides do in fact adjoin. This is difficult because unlike an interval tree, only the start of the stride is ordered in the binary search tree.
		
		\subsection{Checking conflicts}
			
			In SD$^3$, there is a single procedure that evaluates memory conflicts using the point and stride tables. We broke this into two procedures for performance reasons.  The first procedure, IsParallel, is a fast operation that gives a binary answer, either the loop is parallel, or not. The second, slower operation, FindConflictingInstructions, finds the particular instructions which accessed the conflicting memory. Figure \ref{fig:conflict-check} shows the general outline of the relevant operations and data structures.
			
			\begin{figure}
				\caption{Conflict-Check outline}
				\label{fig:conflict-check}
				\includegraphics[scale=1.0]{conflict-diagram.pdf}
			\end{figure}
		
		
		\subsubsection{Truncation of FindConflictingInstructions}

			To ensure efficiency, whether the loop is has conflicts or not, FindConflictingInstructions cannot be run too many times. We reduce the number of times it runs to a constant number per loop instance by making the following observations; 
			
			If the loop iteration does not have any conflicts (discovered by running IsParallel), then we know that no instructions have conflicts, and we have no reason to run FindConflictingInstructions. 
			
			If the loop does have conflicts, then the same conflicts most likely occur in many other loop iterations. So after we have found the locations of conflicts in some constant number of loop iterations, we simply do not check later iterations. The information reported may be incomplete, but some conflicts will always be reported.% We accept the imperfection of reporting 
				%The conflicting instructions are simply guides to the programmer. As such they do not have to be 100\% accurate. In particular, if there are some instruction conflicts which are reported, and others which are not, this is not a serious problem. So this information can be truncated.  
			
			Combined, these two truncation techniques mean that we have reasonably accurate reporting while only running FindConflictingInstructions a constant number of times for a particular loop instance.
			%more than a constant number of times in a loop 
		
		\subsubsection{IsParallel}
		
			The AccessSet makes this operation simple and fast to compute. The intersection of the history write set and the pending read set is checked. The loop iteration is parallel if and only if that intersection is empty. 
			
		\subsubsection{FindConflictingInstructions}
		
			FindConflictingInstructions is broken into two steps. First, lists of strides and points are gathered by using \texttt{extract}. Then, these strides and points are treated as intervals, and the intervals that overlap are found in FindOverlapping. Finally, whether the strides and points actually overlap is calculated in FilterConflicting.
			
		\subsubsection{Find Overlapping}
		
			FindOverlapping finds pairs of overlapping intervals. This is another well known interval problem. We use a sorted list technique. 
			The idea behind the algorithm is to sort the lists of intervals by their first element, and then going through the two lists while storing information about one of the lists. 
			
			Algorithm \ref{algo:findoverlapping} gives a precise description of this algorithm. It takes in two lists, $A$, and $B$, and immediately sorts them. $A$ is simply traversed sequentially. $B$ is traversed as $A$ is traversed while keeping some properties true. These properties are as follows. The current interval of $B$ is the first interval that starts after the interval from $A$ starts (the \texttt{ib} variable in Algorithm \ref{algo:findoverlapping}). It also stores the intervals in $B$ that started before $A$ starts, and end before it ends (the \texttt{before\_last}).
			
		 	From this, all the intervals from $B$ that overlap with the current \texttt{interval} from $A$ can be found quickly: The entire contents of \texttt{before\_last} overlaps with the \texttt{interval} from $A$. All intervals from $B$ which start before \texttt{interval} ends also overlap.
			
			%One of these lists will be gone through sequentially, call this list A.  go through one of the sorted lists, smallest to greatest, and moving through the the other list.  
			
			%a sliding window technique on sorted lists of intervals to compute a list of pairs of overlapping intervals. Call these lists A and B. The algorithm first sorts the two lists of intervals by their start. It considers each interval from A one at a time, call this interval $I$. It keeps a window of intervals of list B which overlap the start of $I$. Each time another interval from A is checked, the algorithm it drops the intervals which end before the new interval starts, and adds in all the intervals which start before the new interval starts, but end after it starts. Then it finds the intervals from B which start after $I$ starts, but before $I$ ends. Algorithm \ref{algo:findoverlapping} has psuedocode for this algorithm.
                    
\begin{algorithm}
    \caption{Find Overlapping}
    \label{algo:findoverlapping}
\begin{verbatim}
find_overlapping_intervals(A : interval list, B : interval list)
    sort A by first
    sort B by first
    overlap = Set()
    // before_last contains all interval from B that start before the current interval from A
    // starts and end after the current interval ends
    before_last = vector() 
    // ib is the first interval of B which starts after the interval from A starts. 
    ib = 0
    for interval in A
        //
        new_before_last = vector()
        for item in before_last:
            if item.last >= interval.first:
                new_before_last.append(item)
        before_last = new_before_last
        
        //Find the ib that has the property for the current interval
        increment ib until B[ib].first >= interval.first
        
        if B[ib].last >= interval.first
            before_last.append(B[ib])

        //report overlaps for the intevals in before_last
        for bl in before_last
            overlap.add(bl,interval)

        //report overlaps for the intervals which start after interval starts
        for ib2 from ib until B[ib2].first > interval.last
            overlap.add(B[ib2],interval)
\end{verbatim}
\end{algorithm}
		
		
		\subsubsection{Filter Non-conflicting}
		
			Recall that the SD$^3$ paper uses the efficient Dynamic-GCD algorithm to check if overlapping strides really do conflict. We found it difficult to work out the details of this algorithm, so instead, we decided to use other methods. In particular, the vast majority of strided conflicts will be much easier to check for conflicts. Many strides will be dense, meaning every byte in between the start and end of the interval will be accessed. Many other possible conflicting strides will have the same stride distance. And most of the remaining strides will have long interval overlap compared to the stride distances, which means that the ordinary GCD test gives the correct answer.
			%Our method simply checks if these cases apply, and if so, calculates the answer.
			The few remaining cases can be checked with a slow algorithm which compares the strides element by element.
			 
			%In particular, it is important to note that having constant time comparison is not needed for algorithmic efficiency of the method. After all, each element of the stride is added one at a time, and it is only compari
			
		\subsection{Loop termination}
		
			When a loop terminates, and it is not the outermost active loop, then it must be merged into its parent loop. Each Point and Stride is extracted from the outer loop's history table, and unless it is killed, it is added to the inner loop's pending table using the add method described in Section \ref{s:add-accesses}. 
		
		\subsubsection{Killing}
		
			Recall that when a history table is merged into the outer loop's pending table, killed addresses must not be added to the pending table. 
			The SD$^3$ algorithm removes killed points and strides by a similar algorithm to their conflict-check method. We used AccessSets to do this instead. Given a particular point or stride, we simply use the AccessSet to check if the bytes that point or stride describe are killed. Strides and points are only removed if every byte they reference is killed. This could result in false positive dependencies, but it seems unlikely that partially killed points and strides are an important case. 
%			
%			AccessSets are first killed with set subtraction, and then unioned into the appropriate set. 
%			Stride and Point tables are merged by simply extracting every Point or Stride from the source table, and adding them one at a time into the destination table using the add method . 
%			
		\subsection{AccessSet implementation}\label{s:access-set}
			
			The AccessSet stores a set of memory addresses. This is in contrast to Point and Stride tables, which also store the instruction associated with each memory access. We found it necessary to store this separate table which stores strictly less information as an optimization. It is critical in reducing the number of inefficient Find-Overlapping operations that are needed. It is also helpful for quickly checking killed memory locations. 
			
			%In particular, it allows us to improve the performance of certain operations which do not need instruction information. 
			
			Since memory addresses are simply integers, we will discuss the implementation in generic terms of an integer set, rather than discussing memory addresses. To store this integer set efficiently, we use bit compression as discussed in Section \ref{s:bit-compress}.
			
			\begin{figure}[H]
				\caption{AddressSet data structure}
				\label{fig:bitset}
				\includegraphics[scale=0.8]{bit_data.pdf}
			\end{figure}
			
			%The Bit table stores a set of overall memory accesses, independently of the instruction it was accessed.
			
			%Points and strides store which instruction accessed them. But some operations do not need that information, they only need the overall memory accesses in the scope, independently of the instruction. We found it advantageous to store this information as well. It is implemented as a compressed bit set, described loosely in section 1.6. Here is a more precise description of the set's data structure:
			
		
			Each of the operations are a combination of the obvious implementation for sparse sets, and the implementation for dense sets. Take in-place set intersection as a representative example. In a sparse set, in-place set intersection can be implemented by iterating through each element of the destination set, and deleting it if the element is not in the source set. In dense sets, the intersection is simply the boolean logical operation $x = x \land y$, applied to every bit. This can be implemented easily and quickly in C with the \texttt{\&} operator. Set intersection is implemented in the bit-set described above by treating the outer set as a sparse set, and the bit array as a dense set. See Algorithm \ref{bitsetintersect} for details. % going though each element of the source set, deleteing it is not in the destination set
			
			\begin{algorithm}
				\caption{BitSetIntersect}
				\label{bitsetintersect}
				\begin{algorithmic}[1]
					\Function{BitSetIntersection}{Dest,Src}
					
					\For{$\text{block},\text{bit-arr} \in \text{Dest}$}
					
					\If {$\text{blocknum} \in \text{Src}$}
					\State { Src.remove(blocknum)}
					\Else
					\State { bit-arr \texttt{\&=} Dest.get(blocknum)}
					\EndIf
					\EndFor
					
					\EndFunction
				\end{algorithmic}
			\end{algorithm}
			
			
			%The Bit table stores overall memory accesses, independently of the instruction it was accessed.
			%This is in contrast to Point and Stride tables, which do store the instruction associated with each memory access. We found it necessary to store this separate table which stores strictly less information for performance reasons. In particular, it allows us to improve the performance of certain operations which do not need instruction information. 
			
			
			%The Bit Table stores information about the memory accesses independently from the instruction at which it was accessed, whereas 
			
			
			%The LoopInstance object, which keeps track of all information for a loop, has separate tables for reads and writes
		
			%The data structures which implement point and stride tables are optimized for efficient merging, instead of efficient conflict checking. The details of their data structures will be explained when explaining merging. They have two methods, an "add" method, which simply adds an item to the table, and an extract method, which gets all current items in the table.
			
			
			%\includegraphics[scale=0.6]{FullLoopData.pdf}
					
		\section{Bit Compression Implementation}
			
			Stride compression introduces all sorts of additional complexity, creates performance issues, and it does not guarantee good compression on all workloads. Bit compression (introduced in Section \ref{s:bit-compress}) promises to be the basis of a much simpler, faster compression system with less reliance on program specific characteristics. 
			
			%This approach may not have been suitable for Kim et. al.
			%Unlike stride compression, it cannot even approximate
			
			\subsection{Main ideas}
			
			Similarly to the naive method, accesses are grouped by their memory address, and instruction information is linked to. But instead of individual memory locations having this information, whole blocks of memory do. %have instructions linked to.
			
			Similar to the bit compressed set described in Section \ref{s:access-set}, the table splits the address space into fixed width blocks, and stores the accesses in that block if there are any (second row of Figure \ref{fig:bit-idea}). Unlike that set, for each block, it also stores a map of instructions to the data which that specific instruction stored (third row of Figure \ref{fig:bit-idea}). Call of the block information together a BlockInfo. Call a fixed length bit array a BlockSet. 
			
			\begin{figure}[h]
				\caption{Overall idea}
				\label{fig:bit-idea}
				\includegraphics[scale=0.85]{BlockSet_accesses.pdf}
			\end{figure}
			
			\subsection{Data structures}
			
			Similarly to the stride compression implementation, pending and history table is divided into two tables, one for reads and one for writes. %This table is organized by memory address block, meaning each key is a memory address
			
			%If a particular block of memory is never accessesed, then nothing representing that block is never put in the set. 
			
			All information about that block of memory is stored in a BlockInfo object, which is shown in Figure \ref{fig:block-data}. BlockInfo stores two thing: a set of overall memory accesses in the block, and a map of instruction addresses to the accesses performed by that instruction in that block. Only instructions which access something in the block are stored.

%			\begin{figure}[h]
%				\caption{CompressedTable}
%				\label{fig:bit-data}
%				\includegraphics[scale=1.0]{bit-data-structure.pdf}
%			\end{figure}
		
			\begin{figure}[h]
				\caption{BlockInfo}
				\label{fig:block-data}
				\includegraphics[scale=1.0]{block-info.pdf}
			\end{figure}
%						
%			
%			\begin{figure}[h]
%				\caption{BlockInfo}
%				\label{fig:block-info-data}
%				\includegraphics[scale=0.8]{bitCompresslabeled.pdf}
%			\end{figure}
			
			\subsection{Adding new memory}
			
			Adding new memory is strait-forward
			When a byte of memory is accessed by an instruction, then:
			
			\begin{enumerate}
				\item If there is no BlockInfo that covers the address in the table, then add an empty one.
				\item Add the byte to the union part of the BlockInfo.
				\item If the instruction is not in the BlockInfo, add it.
				\item Add the byte to the BlockSet associated with the instruction.
			\end{enumerate}
			
			% the access is added to the BlockInfo, (the BlockInfo is put in the table if there is none there). 
			%is put in the accessed block of memory is first accessed,
			
			\subsection{Conflict-check}
			
			When the pending table is merged into the history table, pairs of conflicting instructions need to be found. 
			
			\begin{enumerate}
				\item Find the blocks in the read table of the pending table which overlap with blocks from the write table of the history table. 
				\item If the intersection of the "union" property of the BlockInfo is empty, then the blocks do not conflict.
				\item Otherwise, check each pair of instruction BlockSets in the BlockInfo to see which instructions do conflict. 
			\end{enumerate}
			
			
			\subsection{Merge}
			
			Merging tables is as simple as merging the sets of instructions, and the BlockSets associated with them. 
			
			
\chapter{Results}
	
	\section{Intro}
	
		\subsection{Running applications}
		
		Table \ref{all-applications-table} is a complete table of all the applications which data was collected for, and the command line arguments which were used during collection. 
		
		Note that when the command line argument is a filename, it does not include the full path to the data file. The data files that are used are all ones that were included in the original benchmark, no new files were created. 
		
		
		\begin{table}
			\caption{Benchmark application and their inputs}
			\label{all-applications-table}
			\small
			\begin{tabular}{ |l|l|l| }
				\hline
				Application          & Suite   & Command line arguments  \\ \hline
				\hline
				B+tree & Rodinia  &  \texttt{b+tree.out core 2 file mil.txt command command.txt}  \\ \hline
				Back Propagation & Rodinia  &  \texttt{backprop 655360} \\ \hline
				Breadth First Search & Rodinia  &  \texttt{bfs 4 graph1MW\_6.txt}  \\ \hline
				%CFD & Rodinia  &    \\ \hline
				Heartwall & Rodinia  & \texttt{heartwall test.avi 5 4} \\ \hline
				Hotspot & Rodinia  &   \texttt{hotspot 1024 1024 2 4 temp\_1024 power\_1024 output.out} \\ \hline
				Hotspot3d & Rodinia  & \texttt{3D 512 8 100 power\_512x8 temp\_512x8 output.out}   \\ \hline
				kmeans & Rodinia  &    \texttt{kmeans -i kdd\_cup} \\ \hline
				lavaMD & Rodinia  &  \texttt{lavaMD -cores 4 -boxes1d 10}   \\ \hline
				%leukocyte & Rodinia  &     \\ \hline
				LU Decomposition & Rodinia    &  \texttt{lud\_omp -s 2000} \\ \hline
				Myocite & Rodinia  &  \texttt{myocyte.out 100 1 0 4}  \\ \hline
				Nearest Neighbor & Rodinia  &  \texttt{nn ./nn/filelist\_4 5 30 90}   \\ \hline
				Needleman-Wunsch & Rodinia  & \texttt{needle 2048 10 2}    \\ \hline
				ParticleFilter & Rodinia  &    \texttt{particle\_filter -x 128 -y 128 -z 10 -np 10000} \\ \hline
				PathFinder & Rodinia  &  \texttt{pathfinder 100000 100}   \\ \hline
				SRAD & Rodinia  &  \texttt{srad 2048 2048 0 127 0 127 2 0.5 2}   \\ \hline
				StreamCluster & Rodinia  &    \texttt{sc\_omp 10 20 256 4096 4096 1000 none output.txt 4} \\ \hline
				Black Scholes & Parsec  &  \texttt{blackscholes 1 in\_64K.txt prices.txt} \\ \hline
				Canneal & Parsec  &  \texttt{canneal 1 15000 2000 400000.nets 128} \\ \hline
				Facesim & Parsec  &  \texttt{facesim -timing -threads 1} \\ \hline
				Fluidanimate & Parsec  &  \texttt{fluidanimate 1 5 in\_300K.fluid out.fluid} \\ \hline
				Streamcluster & Parsec  &  \texttt{streamcluster 10 20 128 16384 16384 1000 none output.txt 1} \\ \hline
				Swaptions & Parsec  &  \texttt{swaptions -ns 64 -sm 40000 -nt 1} \\ \hline
			\end{tabular}
		\end{table}
		
	\section{Memory improvement of compression}
		
		%Much of the work of this thesis was the implementation of two compressed versions of pairwise method: stride compression, based on SD$^3$, and bit compression. The native method was also implemented for comparison. 
		Recall that the effect of stride compression, and to a lesser extent bit compression, are contingent on the program characteristics. To make sure that the compression methods are effective, and to see how the two compression methods compare, we measured the memory consumption of these methods on the applications. 
		
		\subsubsection{Memory evaluation setup}
		
		The memory measurement technique used is not perfect. The maximum memory consumption was measured by querying the Unix \texttt{ps} utility approximately every 20ms over the entire run of the program. As such, the resulting memory consumption is only an approximation. In particular, if the program allocated memory, and immediately freed the memory before \texttt{ps} was queried again, that additional memory would not be measured. However, that measurement error could only have significant impact on programs with small memory consumption and run quickly, which certainly do not describe the pairwise method implementations.
		
		\subsubsection{Inadvisablilty of the naive method}
		
		
		\begin{figure}
			\caption{Memory compression of applications}
			\label{fig:mem-comp-plot}
			\includegraphics[scale=0.7]{plots/overall_plot.png}
		\end{figure}
		
		The naive method often took up more memory than an ordinary machine would have. The full results are plotted in Figure \ref{fig:mem-comp-plot}. Our system had 16gb of memory, and so the process was terminated after the program used over 14gb (on the plot, this is marked by an X). The naive method used more that 14gb on 11 of the 21 applications, while neither the stride nor the bit compression methods used that much on any of the applications. This result confirms Kim et. al.'s observations that the naive method is not useful for important practical purposes, and that stride compression does help solve the problem.\footnote{\cite{Kim:2010}}
		
		%Next, note the huge variation in the absolute and relative overhead of all three implementations of the pairwise method. Sometimes the 
		
		%One question to ask is whether the stride method, with its additional complexity, but better theoretical compression, is better than the simpler and faster bit compression. 
		
		\subsubsection{Bit compression vs. Stride Compression}
		
		Next consider the respective memory overhead of stride compression and bit compression. 
		Recall that in theory, stride compression can also do much better than bit compression. In the best case, stride compression can make profiling memory consumption independent of input size, as the entire working memory can be compressed into a single stride. Of course, this ideal scenario is highly unlikely in real programs, and in the worst case, little memory is accessed in a strided manner, and stride compression does no better than the naive version. Bit compression claims to be less dependent on program characteristics, as it stores blocks of contiguous accesses, but it can also do as poorly as the naive method in cases where different instructions access the same blocks of memory. So determining which method is better needs to be measured on real applications. 
		
		As a caveat, note that this stride implementation is not completely comparable to SD$^3$, since we added an AccessSet, which SD$^3$ does not store. % this set does not store instruction information, this table is much less memory intensive than the complete. %Although it's indepen bit table is stored as well as the stride table. 
		
		Overall, the performance of the methods depends on the applications. Bit compression sometimes performs better and sometimes worse than the stride version. For example, stride compression gets better results on srad, heartwall, and backprop, while bit compression does better on bfs and fluidanimate. But when bit compression performs better, it sometimes performs much better, while stride compression only does better by a little. For example, srad is the application where stride performance beats bit compression by the greatest margin, only 47\% of the memory of the bit compression. Meanwhile, for bfs, bit compression uses only 6.2\% of the memory of stride compression, a huge improvement.
		
		To see why bit compression can do so much better than stride compression, consider the case of bfs. The runtime of bfs is dominated by processing the text input, which is a list of integer pairs that represent graph edges. For example, take the following:
		
		\begin{verbatim}
		2541 3
		3290 5
		1114 6
		3576 4
		3647 4
		\end{verbatim}
		
		%Text processing is not particularly stride friendly, since different instructions handle different parts of the text. 
		This text will not be processed with highly strided acceses.
		Assuming an ordinary implementation of text processing, the method to process the integer \texttt{2541} will exit when the space is hit, and then will start again when processing the integer \texttt{3}. But these will be stored as two short strides, not one, since the space in between the intergers is skipped. Bit compression stores whole blocks of memory, so the two sequences of accesses will most likely be put in the same block, and also the accesses created by processing the next few lines. %The actual implementation uses repeated calls to fread, but presumably under the covers something like this is going on.
		
		The main process of bfs is also not stride friendly. In the bfs code, the graph is implemented as an adjacency list. The inner loop of bfs traverses the edge list of a node, and touches another array based on the location specified by the edge. Code example \ref{code:bfs} is the inner loop of bfs copied from the bfs source code. The \texttt{h\_graph\_edges} array stores the edge list, and the value it stores is used as an index into the \texttt{h\_graph\_visited} array. Since the locations at which the memory is access is dependent on the values in the input, the accesses to the \texttt{h\_graph\_visited} will not be a stride. 
		
		
\begin{codeexample}
	\caption{Breadth first search inner loop}
	\label{code:bfs}
		\begin{verbatim}
for(int i=h_graph_nodes[tid].starting; 
        i<(h_graph_nodes[tid].no_of_edges + h_graph_nodes[tid].starting); 
        i++) {
    int id = h_graph_edges[i];
    if(!h_graph_visited[id]) {
        h_cost[id]=h_cost[tid]+1;
        h_updating_graph_mask[id]=true;
    }
}
		\end{verbatim}
	\end{codeexample}
		
		Another surprising result is that the naive method uses comparable memory to the compressed methods on several applications, most notably Particle Filter and Needleman-Wunsch. This suggests that there are some applications which neither compression method works effectively on. 
		
		%This give evidence for the possibility that bit compression gives more reliable compression ratios than stride compression. To 
		
		%that the bit compression method is often 
		
		\subsection{Future work}
		
		Applications are continuing to get more and more memory intensive, and so we will continue to need better memory compression methods to support that growth. We were considering using the SPEC 2017 applications, but found that some of applications on "train" inputs were consuming too much memory. The deepsjeng\_s application used almost 7gb of memory on intspeed.
		xz 900mb
		gcc 600mb
		
		fpspeed
		bwaves 800mb
		lbm\_s\_base.myte 3.1gb
		speed\_pop2\_base 1.4gb
		
		These memory numbers far exceed that of the SPEC 2006 memory consumption on train inputs. 
		
		One possiblilty to improve compression ratios is to combine the stride and bit compression. This can be done by implementing the point table using bit compression, and possibly by putting small strides in the point table when they can't be merged in with larger strides. 
		
	
	\section{Identifying important parallelism}
		
		One of the goals of this thesis is to use parallelism detection to aid further analysis, for example estimating the effectiveness of GPU acceleration. 
		This section evaluates whether the parallelism tool we built can be used for this purpose. In particular, we confirm that the tool does detect loops which could possibly be accelerated by massively parallel hardware for meaningful performance improvements. 
		%In order for an analysis to be meaningful, the loops detected as parallel need to be important to application performance, and need to be scalable on many cores. %However, such an analysis would not work if  
		%However, that will not be possible if the loops detected as parallel are not important for application performance. %  important loops, those that can be 
		%In order for a parallelism detector tool to be useful, 
		
		%The effectiveness of the tools is questionable because it only detects trivially parallel loops: loops where each iteration is completely independent. Loops which can be parallelized with reductions or with locking mechanisms are not always identified as parallel. But we hope that this method will identify significant parallelism in many applications that can be parallelized on GPUs. % and will also help the programmer parallelize loops detected as not parallel. 
		
		\subsubsection{Significance of loops}
		
		Most loops are not worth investigating, and should be eliminated from further analysis. 
		Programmers usually want to optimize entire applications; individual loops are only optimized when significant proportions of the application's running time is in the loop. 
		%We found that certain loops, some parallelizable and some not, have outsized importance to program runtime. 
		Figure \ref{fig:size-iteration-plots} shows the percentage of running time spent in the loops (estimated by machine instructions executed), how many iterations they have, and whether they were found to not have data conflicts (which means they are parallelizable). This plot includes all loops from all applications. 
		%In the figure, loops which are parallel, and have a large number of iterations (blue dots, upper left) have a good chance of being usefully parallelized on GPUs. 
		%Loops which are not found to be parallel, but do have a large number of iterations, and are important (red dots, upper left) can be reported to the programmer as worthy of further investigation. Perhaps they can be parallelized with a reduction or with some data synchronization. 
		
		%In this plot, as with the all of our analysis, running time is estimated with number of machine instructions executed. 
		%We use the proportion of instructions evaluated in the loop to estimate how much time is spent in a loop. % we estimate the percentage of time spent in the loop. This is estimated b calculate the %evaluate how important  is a metric for loop importance relative \
		%This metric is placed as a function of loop iterations is displayed on Figure . The higher the loop is on the graph, the more important it in the application performance.
		
		\begin{figure}
			\caption{Loop importance and parallelizability}
			\label{fig:size-iteration-plots}
			\includegraphics[scale=0.7]{plots/size_iteration_plot.png}
		\end{figure}
		
		This plot shows that there are several loops which can be trivially parallelized for significant application speedup. 
		These are the loops which are parallel (do not have conflicts) and have substantial number of iterations (blue dots, upper left). These can be reported to the programmer as targets of trivial parallelization. 
		
		The important loops that are not parallel (have conflicts) but have a large number of iterations are also important (red dots, upper left). These loops could possibly be parallelized with a reduction or with data synchronization, and should be reported to the programmer for further investigation.
		
		But the dots near the bottom are unimportant to application performance, and so should be ignored. This eliminates most of the loops from consideration. 75\% of the loops include less than 1\% of the total runtime of their respective application, and 83\% of loops include less than 10\% of application runtime). 
		
		%And the dots on the far left cannot be parallelized to a high degree.
		% These loops could be reported to the programmer as ones that should be investigated for non-trivial parallelization.
		
		%From this plot, you can see that there are many important loops, some of which are parallel, and some of which are not, some of which have many iterations, some of which have few.
		%This
		
		\subsubsection{Parallelizability of applications}
		
		The applications profiled are all parallelizable to some degree, and many of them can be run effectively on GPUs. Does the tool detect all these applications as having significant potential to parallelize?
		
		To answer this, we introduce a metric for loop speedup which assumes a sort of idealized parallelism. It is calculated with:
		$$\text{loop speedup} = \text{loop size} - \frac{\text{loop size}}{\text{iterations per instance}} $$
		Where the loop size is the proportion of time (approximated by instruction count) spent inside the loop.
		This speedup estimate assumes no overhead, infinite cores, and uniform loop iterations.
		These assumptions are extremely optimistic, and so this estimate of loop speedup may also be overly optimistic. But it gives a rough estimate of parallizabililty. Many of the loops which score highly, for example, the main loop of blackscholes, are in fact highly parallelizable. The loops which score poorly, are certainly not useful to parallelize. 
		So this metric can be interpreted as an upper bound on how much the loop can reduce application performance if parallelized. 
		
		
		\begin{figure}
			\caption{Performance improvement by idealized parallelization}%Possible speedups with parallelization}
			\label{fig:application-parr}
			
			\begin{minipage}{0.73\textwidth} % choose width suitably
				\includegraphics[scale=0.9]{plots/application_idealized_speedup.png}
				{\footnotesize Speedup by parallelizing a particular loop in the given application assuming no overhead and infinite cores\par}
			\end{minipage}
		\end{figure}
		
		We detected some substantial parallelism in most of the profiled applications. 
		Figure \ref{fig:application-parr} shows speedup of entire applications assuming idealized parallelization of a single loop in the program. BFS, hotspot and lavaMD show little potential for parallelism, as none of the parallel loops are very important to the application. Others, for example heartwall, LUD, and blackscholes show promise for large performance gains by parallelizing a single loop. Others are more ambiguous, such as swaptions and fluidanimate, because while parallelizing a single loop cannot increase application performance that much, it is possible that parallelizing several loops can in fact improve performance substantially. Since this plot only gives information about a single, loop, it cannot say either way.
		
		\subsubsection{Uniformity of useful parallel loops}
		
		It greatly simplifies analysis to assume that important parallel loops are uniform, i.e., all the loop iterations do the same amount of work. While this is true for most loops, some loops have substantial and problematic variability in iteration size that can impact parallelizability. 
			
		To see why this is a problem, consider the idealized parallel machine where infinite threads can run in parallel. Consider a loop with 1000 iterations that can run in parallel, but one of those loop iterations takes as much time all the others combined. In this case, overall loop performance can at most be doubled. But if you assume uniformity of iterations, then performance would be calculated to increase by 1000 times, so the actual and predicted speedup would be off by a factor of 500. 
		%So non-uniform loops have serious implications to parallelizability. %This is even more important in the case of massively parallel hardware, like GPUs, since each thread is slower than a GPU thread. 
		
		Luckily, for the applications chosen, most loops are roughly uniform. Figure \ref{fig:uniformity-tail-etc} shows a metric of uniformity plotted over the number of iterations per loop instance. The metric chosen is the maximum iteration size over the mean size. Only loops which could speed up the program by 10\% or more assuming idealized parallelism are shown. 
		
		The two dots in the top left of the graph are loops which have serious problems for parallelizability. They only have 10 loop iterations each on average, but the maximum iteration size is 10 times the mean iteration. This means that performance may not be increased substantially by parallelization, since one iteration dominates the runtime. 
		
		The loop that appears in the top right of the graph is not so problematic. After all, it has over 10,000 loop iterations. If the iterations are carefully distributed over threads, it is possible that the variance between iteration sizes would not have a significant impact on performance, since the smaller iterations could be coalesced together into threads comparable to the large thread.
		
		The loops which show moderate amounts of variance need to be similarly evaluated on a case by case basis. But 67\% of the important loops shown, the max iteration size is less than 10\% greater than the mean iteration size, which means that most of the important loops are in fact uniform. 
		
		%So a value of 10 means that the maximum 
		
		%High values indicate some variance in the 
		
		\begin{figure}
			\caption{Uniformity of loop iterations}
			\label{fig:uniformity-tail-etc}
			
			\begin{minipage}{0.6\textwidth} % choose width suitably
				\includegraphics[scale=0.7]{plots/tail_iters.png}
				{\footnotesize Loops which could speed up the program by 10\% or more assuming idealized parallelism \par}
			\end{minipage}
		\end{figure}
		
		\subsubsection{Scale of detected parallelism}
		
		The scale of the parallelism is one of the most important considerations when evaluating what sort of hardware to run the loop on. 
		In particular, 
		CPUs can run a small number of threads quic
		
		\begin{figure}
			\caption{Uniformity of loop iterations}
			\label{fig:uniformity-tail-etc}
			
			\begin{minipage}{0.6\textwidth} % choose width suitably
				\includegraphics[scale=0.7]{plots/categorized_conflicts.png}
				{\footnotesize Loops which could speed up the program by 10\% or more assuming idealized parallelism \par}
			\end{minipage}
		\end{figure}
		
		
		
		%It will be used only to eliminate loops which score poorly from consideration. This will eliminate the many small loops with little importance to the program, and also loops which are not detected to be parallelizable. 
		
		%, or loops with . Loops which score low on this metric will not be useful to parallelize, loops that score high are not necessarily good candidates for parallelization. 
		
		
		
		%first cutlook at how 
		
		%These assumptions can certainly lead to misleading results, it sometimes corresponds to actual parallelization.
		
	
	\section{Matching parallelism with other metrics}
	
		As discussed in Section \ref{s:workload-characht}, choosing the right hardware involves other considerations than just the scale of parallelism. 
		
		\begin{table}
			\caption{Other metrics collected}
		\begin{tabular}{ |p{4.5cm}|p{11cm}| }
			\hline
			Metric name & Metric description \\
			\hline \hline
			SwitchesOverBranches & Number of time a branch path changes direction over the number of times a branch instruction is executed \\ \hline
			SwitchesOverInstructions & Number of time a branch path changes direction over the number of instructions executed \\ \hline
			PercNonStridedAccesses & Percentage of accesses which do not access memory as a stride \\ \hline
			StridedAccessesBelow16 & Percentage of accesses which are strides of length 16 bytes or less \\ \hline
			StridedAccessesAbove16 &  Percentage of accesses which are strides of length 16 bytes or more \\ \hline
			PercFixedAccesses & Percentage of accesses which are strides of length zero, i.e. they access the same location of memory location repeatedly \\ \hline
			MemoryAccessesPerInstr & Number of memory accesses over number of instructions \\ \hline
			PercAccessesReads & Percentage of memory accesses which are reads  \\ \hline
		\end{tabular}
	\end{table}
	
	
	\begin{figure}
		\caption{Other metrics iterations}
		\label{fig:other-metrics}
		
		\begin{minipage}{0.6\textwidth} % choose width suitably
			\includegraphics[scale=0.7]{plots/stats_variablility_plot.png}
			{\footnotesize Loops which could speed up the program by 10\% or more assuming idealized parallelism \par}
		\end{minipage}
	\end{figure}

\chapter*{Conclusion}
         \addcontentsline{toc}{chapter}{Conclusion}
	\chaptermark{Conclusion}
	\markboth{Conclusion}{Conclusion}
	\setcounter{chapter}{4}
	\setcounter{section}{0}

Here's a conclusion, demonstrating the use of all that manual incrementing and table of contents adding that has to happen if you use the starred form of the chapter command. The deal is, the chapter command in \LaTeX\ does a lot of things: it increments the chapter counter, it resets the section counter to zero, it puts the name of the chapter into the table of contents and the running headers, and probably some other stuff.

So, if you remove all that stuff because you don't like it to say ``Chapter 4: Conclusion'', then you have to manually add all the things \LaTeX\ would normally do for you. Maybe someday we'll write a new chapter macro that doesn't add ``Chapter X'' to the beginning of every chapter title.


%If you feel it necessary to include an appendix, it goes here.
    \appendix
      \chapter{The First Appendix}


%This is where endnotes are supposed to go, if you have them.
%I have no idea how endnotes work with LaTeX.

  \backmatter % backmatter makes the index and bibliography appear properly in the t.o.c...

% if you're using bibtex, the next line forces every entry in the bibtex file to be included
% in your bibliography, regardless of whether or not you've cited it in the thesis.
    \nocite{*}

% Rename my bibliography to be called "Works Cited" and not "References" or ``Bibliography''
% \renewcommand{\bibname}{Works Cited}

%    \bibliographystyle{bsts/mla-good} % there are a variety of styles available;
%  \bibliographystyle{plainnat}
% replace ``plainnat'' with the style of choice. You can refer to files in the bsts or APA
% subfolder, e.g.
 \bibliographystyle{APA/apa-good}  % or
 \bibliography{thesis}
 % Comment the above two lines and uncomment the next line to use biblatex-chicago.
 %\printbibliography[heading=bibintoc]

% Finally, an index would go here... but it is also optional.
\end{document}
